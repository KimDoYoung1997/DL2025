{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 14wk-1: (강화학습) – 4x4 Grid World 환경의 이해\n",
        "\n",
        "최규빈  \n",
        "2025-06-04\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/14wk-1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상\n",
        "\n",
        "<https://youtu.be/playlist?list=PLQqh36zP38-xtp-2udbCeLzUvHSvI4X3y&si=vYqsA5Sffbs7C0xD>\n",
        "\n",
        "# 2. Imports"
      ],
      "id": "71843881-6f50-476a-8384-5cbd91832a90"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "#---#\n",
        "import numpy as np\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import IPython"
      ],
      "id": "1d667404-aced-49db-b8c1-bb4e41eba4f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 지난시간 코드 복습\n",
        "\n",
        "`-` 클래스선언\n",
        "\n",
        "-   수정사항: (1) deque의 maxlen을 500000 으로 조정 (2) print하는 코드를\n",
        "    주석처리"
      ],
      "id": "930cea0d-277a-493a-877b-7838b0dd24aa"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.a2d = {\n",
        "            0: np.array([0,1]), # →\n",
        "            1: np.array([0,-1]), # ←  \n",
        "            2: np.array([1,0]),  # ↓\n",
        "            3: np.array([-1,0])  # ↑\n",
        "        }\n",
        "        self.state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "        self.state = np.array([0,0])\n",
        "        self.reward = None\n",
        "        self.terminated = False\n",
        "    def step(self,action):\n",
        "        self.state = self.state + self.a2d[action]\n",
        "        s1,s2 = self.state\n",
        "        if (s1==3) and (s2==3):\n",
        "            self.reward = 100 \n",
        "            self.terminated = True\n",
        "        elif self.state in self.state_space:\n",
        "            self.reward = -1 \n",
        "            self.terminated = False\n",
        "        else:\n",
        "            self.reward = -10\n",
        "            self.terminated = True\n",
        "        # print(\n",
        "        #     f\"action = {action}\\t\"\n",
        "        #     f\"state = {self.state - self.a2d[action]} -> {self.state}\\t\"\n",
        "        #     f\"reward = {self.reward}\\t\"\n",
        "        #     f\"termiated = {self.terminated}\"\n",
        "        # )            \n",
        "        return self.state, self.reward, self.terminated\n",
        "    def reset(self):\n",
        "        self.state = np.array([0,0])\n",
        "        self.terminated = False\n",
        "        return self.state\n",
        "class RandomAgent:\n",
        "    def __init__(self):\n",
        "        self.state = np.array([0,0]) \n",
        "        self.action = None \n",
        "        self.reward = None \n",
        "        self.next_state = None\n",
        "        self.terminated = None\n",
        "        #---#\n",
        "        self.states = collections.deque(maxlen=500000)\n",
        "        self.actions = collections.deque(maxlen=500000)\n",
        "        self.rewards = collections.deque(maxlen=500000)\n",
        "        self.next_states = collections.deque(maxlen=500000)\n",
        "        self.terminations = collections.deque(maxlen=500000)\n",
        "        #---#\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "        self.n_experience = 0\n",
        "    def act(self):\n",
        "        self.action = self.action_space.sample()\n",
        "    def save_experience(self):\n",
        "        self.states.append(self.state)\n",
        "        self.actions.append(self.action)\n",
        "        self.rewards.append(self.reward)\n",
        "        self.next_states.append(self.next_state)\n",
        "        self.terminations.append(self.terminated)\n",
        "        self.n_experience = self.n_experience + 1\n",
        "    def learn(self):\n",
        "        pass"
      ],
      "id": "51ea8f17-ff1a-47a6-b30e-9c2e0ca9fdfd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 메인코드\n",
        "\n",
        "-   수정사항: 가독성을 위해 에피소드가 진행되는 for문의 구조를 수정함\n",
        "    (특히 step4)\n",
        "\n",
        "``` python\n",
        "player = RandomAgent()\n",
        "env = GridWorld()\n",
        "scores = [] \n",
        "score = 0 \n",
        "#\n",
        "for e in range(1,100):\n",
        "    #---에피소드시작---#\n",
        "    while True:\n",
        "        # step1 -- 액션선택\n",
        "        player.act()\n",
        "        # step2 -- 환경반응 \n",
        "        player.next_state, player.reward, player.terminated = env.step(player.action)\n",
        "        # step3 -- 경험기록 & 학습 \n",
        "        player.save_experience()\n",
        "        player.learn()\n",
        "        # step4 --종료 조건 체크 & 후속 처리\n",
        "        if env.terminated:\n",
        "            score = score + player.reward\n",
        "            scores.append(score)\n",
        "            score = 0 \n",
        "            player.state = env.reset() \n",
        "            print(f\"---에피소드{e}종료---\")\n",
        "            break\n",
        "        else: \n",
        "            score = score + player.reward\n",
        "            scores.append(score)            \n",
        "            player.state = player.next_state\n",
        "    #---에피소드끝---#\n",
        "    if scores[-1] > 0:\n",
        "        break\n",
        "```\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> 마음에 들지 않지만 꼭 외워야 하는것\n",
        ">\n",
        "> 1.  `env.step`은 항상 next_state, reward, terminated, truncated, info\n",
        ">     를 리턴한다. – 짐나지엄 라이브러리 규격때문\n",
        "> 2.  `env.reset`은 환경을 초기화할 뿐만 아니라, state, info를 반환하는\n",
        ">     기능도 있다. – 짐나지엄 라이브러리 규격때문\n",
        "> 3.  `player`는 항상 `state`와 `next_state`를 구분해서 저장한다.\n",
        ">     (다른변수들은 그렇지 않음) 이는 강화학습이\n",
        ">     MDP(마코프체인+행동+보상)구조를 따르게 때문에 생기는 고유한\n",
        ">     특징이다. – 이론적이 이유\n",
        "\n",
        "`-` 환경과 에이전트의 상호작용 이해를 위한 다이어그램:\n",
        "\n",
        "-   <https://claude.ai/public/artifacts/7fad72b5-0946-47bd-a6cd-b33b21856590>\n",
        "\n",
        "# 4. GridWorld 환경의 이해\n",
        "\n",
        "## A. 데이터 축적\n",
        "\n",
        "`-` 랜덤에이전트를 이용해 무작위로 100,000 에피소드를 진행해보자."
      ],
      "id": "09051907-fc5f-4464-863f-5f514dfaab30"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "player = RandomAgent()\n",
        "env = GridWorld()\n",
        "scores = [] \n",
        "score = 0 \n",
        "#\n",
        "for e in range(1,100000):\n",
        "    #---에피소드시작---#\n",
        "    while True:\n",
        "        # step1 -- 액션선택\n",
        "        player.act()\n",
        "        # step2 -- 환경반응 \n",
        "        player.next_state, player.reward, player.terminated = env.step(player.action)\n",
        "        # step3 -- 경험기록 & 학습 \n",
        "        player.save_experience()\n",
        "        player.learn()\n",
        "        # step4 --종료 조건 체크 & 후속 처리\n",
        "        if env.terminated:\n",
        "            score = score + player.reward\n",
        "            scores.append(score)\n",
        "            score = 0 \n",
        "            player.state = env.reset() \n",
        "            break\n",
        "        else: \n",
        "            score = score + player.reward\n",
        "            scores.append(score)            \n",
        "            player.state = player.next_state"
      ],
      "id": "7f808719-bb01-49d2-b064-c7d08c4347e1"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "player.n_experience"
      ],
      "id": "ef29fdf9-b027-4d88-8bd7-1e5c6d3b08d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 첫번째 `q_table`\n",
        "\n",
        "`-` 밴딧게임에서는 $q(a)$ 를 정의했었음.\n",
        "\n",
        "-   $q(0) = 1$\n",
        "-   $q(1) = 10$\n",
        "\n",
        "`-` 여기에서는 $q(s_1,s_2,a)$를 정의해야함!\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> 직관적으로 아래의 그림이 떠오름\n",
        "> ![](https://github.com/guebin/DL2025/blob/main/posts/14wk-1-fig1.png?raw=true)\n",
        ">\n",
        "> 그림에 대응하는 $q(s_1,s_2,a)$의 값은 아래와 같음\n",
        ">\n",
        "> ### $a=0$\n",
        ">\n",
        "> $a=0 \\Leftrightarrow \\text{\\tt action=right}$\n",
        ">\n",
        "> $$ \\begin{bmatrix}\n",
        "> q(0,0,0) & q(0,1,0) & q(0,2,0) & q(0,3,0) \\\\ \n",
        "> q(1,0,0) & q(1,1,0) & q(1,2,0) & q(1,3,0) \\\\ \n",
        "> q(2,0,0) & q(2,1,0) & q(2,2,0) & q(2,3,0) \\\\ \n",
        "> q(3,0,0) & q(3,1,0) &q(3,2,0) & q(3,3,0) \\\\ \n",
        "> \\end{bmatrix} =  \\begin{bmatrix}\n",
        "> -1 & -1 & -1 & -10 \\\\ \n",
        "> -1 & -1 & -1 & -10 \\\\ \n",
        "> -1 & -1 & -1 & -10 \\\\ \n",
        "> -1 & -1 & 100 &  \\text{-} \\\\ \n",
        "> \\end{bmatrix}\n",
        "> $$\n",
        ">\n",
        "> ### $a=1$\n",
        ">\n",
        "> $a=1 \\Leftrightarrow \\text{\\tt action=left}$\n",
        ">\n",
        "> $$ \\begin{bmatrix}\n",
        "> q(0,0,1) & q(0,1,1) & q(0,2,1) & q(0,3,1) \\\\ \n",
        "> q(1,0,1) & q(1,1,1) & q(1,2,1) & q(1,3,1) \\\\ \n",
        "> q(2,0,1) & q(2,1,1) & q(2,2,1) & q(2,3,1) \\\\ \n",
        "> q(3,0,1) & q(3,1,1) &q(3,2,1) & q(3,3,1) \\\\ \n",
        "> \\end{bmatrix} = \\begin{bmatrix}\n",
        "> -10 & -1 & -1 & -1 \\\\ \n",
        "> -10& -1 & -1 & -1 \\\\ \n",
        "> -10 & -1 & -1 & -1 \\\\ \n",
        "> -10 & -1 & -1 &  \\text{-} \\\\ \n",
        "> \\end{bmatrix}\n",
        "> $$\n",
        ">\n",
        "> ### $a=2$\n",
        ">\n",
        "> $a=2 \\Leftrightarrow \\text{\\tt action=down}$\n",
        ">\n",
        "> $$  \\begin{bmatrix}\n",
        "> q(0,0,2) & q(0,1,2) & q(0,2,2) & q(0,3,2) \\\\ \n",
        "> q(1,0,2) & q(1,1,2) & q(1,2,2) & q(1,3,2) \\\\ \n",
        "> q(2,0,2) & q(2,1,2) & q(2,2,2) & q(2,3,2) \\\\ \n",
        "> q(3,0,2) & q(3,1,2) &q(3,2,2) & q(3,3,2) \\\\ \n",
        "> \\end{bmatrix} = \\begin{bmatrix}\n",
        "> -1 & -1 & -1 & -1 \\\\ \n",
        "> -1& -1 & -1 & -1 \\\\ \n",
        "> -1 & -1 & -1 & 100\\\\ \n",
        "> -10 & -10 & -10 &  \\text{-} \\\\ \n",
        "> \\end{bmatrix}\n",
        "> $$\n",
        ">\n",
        "> ### $a=3$\n",
        ">\n",
        "> $a=3 \\Leftrightarrow \\text{\\tt action=up}$\n",
        ">\n",
        "> $$  \\begin{bmatrix}\n",
        "> q(0,0,3) & q(0,1,3) & q(0,2,3) & q(0,3,3) \\\\ \n",
        "> q(1,0,3) & q(1,1,3) & q(1,2,3) & q(1,3,3) \\\\ \n",
        "> q(2,0,3) & q(2,1,3) & q(2,2,3) & q(2,3,3) \\\\ \n",
        "> q(3,0,3) & q(3,1,3) &q(3,2,3) & q(3,3,3) \\\\ \n",
        "> \\end{bmatrix} =\\begin{bmatrix}\n",
        "> -10 & -10 & -10 & -10\\\\ \n",
        "> -1& -1 & -1 & -1 \\\\ \n",
        "> -1 & -1 & -1 & -1 \\\\ \n",
        "> -1 & -1 & -1 &  \\text{-} \\\\ \n",
        "> \\end{bmatrix}\n",
        "> $$\n",
        "\n",
        "`-` 데이터를 바탕으로 $q(s_1,s_2,a)$를 구해보자."
      ],
      "id": "bb62af3a-02e9-426b-a10e-10e8d238cac8"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "player.states[0], player.actions[0], player.rewards[0]"
      ],
      "id": "dc84ffd7-45da-47ae-94d7-16347398f1ee"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_table = np.zeros((4,4,4))\n",
        "count = np.zeros((4,4,4))"
      ],
      "id": "2f78c883-feca-45e1-bbdc-166cf754b4a8"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory =  zip(player.states, player.actions, player.rewards)\n",
        "for (s1,s2), a, r in memory:\n",
        "    q_table[s1,s2,a] = q_table[s1,s2,a] + r\n",
        "    count[s1,s2,a] = count[s1,s2,a] + 1 "
      ],
      "id": "750322b8-bb46-4ac7-87fc-c54f1d3ef782"
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "count[count==0] = 0.001 "
      ],
      "id": "bd64bf9d-eebe-4cc0-abc6-ba4bb934d7c2"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_table = q_table / count"
      ],
      "id": "e035d24e-a05a-45c3-9692-5c20719e7c3c"
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_table[...,0], q_table[...,1], q_table[...,2], q_table[...,3]"
      ],
      "id": "e3c4df1d-8464-4a20-afc0-15d2e10a65a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` count를 사용하지 않는 방법은 없을까? – 테크닉"
      ],
      "id": "94a4c0d6-b701-4f3d-80b0-bb3f7ca5b462"
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_table = np.zeros((4,4,4))\n",
        "memory =  zip(player.states, player.actions, player.rewards)\n",
        "for (s1,s2), a, r in memory:\n",
        "    qhat = q_table[s1,s2,a] # 내가 생각했던갓\n",
        "    q = r # 실제값\n",
        "    diff = q-qhat # 차이\n",
        "    q_table[s1,s2,a] = q_table[s1,s2,a]  + 0.01*diff# update"
      ],
      "id": "323a393f-c8b0-47d0-872c-3e46f37ed682"
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_table.round(2)"
      ],
      "id": "5b395794-2199-42c4-84ae-3c55448b0c16"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. 첫번째 `q_table`보다 나은 것?\n",
        "\n",
        "`-` 첫번째 `q_table`을 알고있다고 가정하자.\n",
        "\n",
        "![](https://github.com/guebin/DL2025/blob/main/posts/14wk-1-fig1.png?raw=true)\n",
        "\n",
        "`-` 정책시각화 (합리적인 행동)\n",
        "\n",
        "![](https://github.com/guebin/DL2025/blob/main/posts/14wk-1-fig2.png?raw=true)\n",
        "\n",
        "`-` 이게 최선의 정책일까?"
      ],
      "id": "936b7c06-9358-41fb-87ca-6ad62c9b3c12"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  }
}