{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 14wk-1: (강화학습) – 4x4 Grid World 환경의 이해\n",
        "\n",
        "최규빈  \n",
        "2025-06-02\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/13wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상\n",
        "\n",
        "<https://youtu.be/playlist?list=PLQqh36zP38-xRfokRN58uC0Mr4NGrJiWX&si=PhVcbmFUN0x3TLfN>\n",
        "\n",
        "# 2. Imports"
      ],
      "id": "01e12f91-6eaa-4767-a4ef-2cad166b2c95"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "#---#\n",
        "import numpy as np\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import IPython"
      ],
      "id": "1d667404-aced-49db-b8c1-bb4e41eba4f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 지난시간 코드 복습\n",
        "\n",
        "`-` 클래스 – 수정사항없음 // 편의상 print하는 코드만 주석처리함"
      ],
      "id": "b3044dc7-5c9e-4f9c-9092-8043a989c73b"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.a2d = {\n",
        "            0: np.array([0,1]), # →\n",
        "            1: np.array([0,-1]), # ←  \n",
        "            2: np.array([1,0]),  # ↓\n",
        "            3: np.array([-1,0])  # ↑\n",
        "        }\n",
        "        self.state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "        self.state = np.array([0,0])\n",
        "        self.reward = None\n",
        "        self.terminated = False\n",
        "    def step(self,action):\n",
        "        self.state = self.state + self.a2d[action]\n",
        "        s1,s2 = self.state\n",
        "        if (s1==3) and (s2==3):\n",
        "            self.reward = 100 \n",
        "            self.terminated = True\n",
        "        elif self.state in self.state_space:\n",
        "            self.reward = -1 \n",
        "            self.terminated = False\n",
        "        else:\n",
        "            self.reward = -10\n",
        "            self.terminated = True\n",
        "        # print(\n",
        "        #     f\"action = {action}\\t\"\n",
        "        #     f\"state = {self.state - self.a2d[action]} -> {self.state}\\t\"\n",
        "        #     f\"reward = {self.reward}\\t\"\n",
        "        #     f\"termiated = {self.terminated}\"\n",
        "        # )            \n",
        "        return self.state, self.reward, self.terminated\n",
        "    def reset(self):\n",
        "        self.state = np.array([0,0])\n",
        "        self.terminated = False\n",
        "        return self.state\n",
        "class RandomAgent:\n",
        "    def __init__(self):\n",
        "        self.state = None \n",
        "        self.action = None \n",
        "        self.reward = None \n",
        "        self.next_state = None\n",
        "        self.terminated = None\n",
        "        #---#\n",
        "        self.states = collections.deque(maxlen=500)\n",
        "        self.actions = collections.deque(maxlen=500)\n",
        "        self.rewards = collections.deque(maxlen=500)\n",
        "        self.next_states = collections.deque(maxlen=500)\n",
        "        self.terminations = collections.deque(maxlen=500)\n",
        "        #---#\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "        self.n_experience = 0\n",
        "    def act(self):\n",
        "        self.action = self.action_space.sample()\n",
        "    def save_experience(self):\n",
        "        self.states.append(self.state)\n",
        "        self.actions.append(self.action)\n",
        "        self.rewards.append(self.reward)\n",
        "        self.next_states.append(self.next_state)\n",
        "        self.terminations.append(self.terminated)\n",
        "        self.n_experience = self.n_experience + 1\n",
        "    def learn(self):\n",
        "        pass"
      ],
      "id": "51ea8f17-ff1a-47a6-b30e-9c2e0ca9fdfd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 메인코드 – 구조를 수정함\n",
        "\n",
        "``` python\n",
        "player = RandomAgent()\n",
        "env = GridWorld()\n",
        "scores = [] \n",
        "score = 0 \n",
        "#\n",
        "for e in range(1,100):\n",
        "    #---에피소드시작---#\n",
        "    while True:\n",
        "        # step1 -- 액션선택\n",
        "        player.act()\n",
        "        # step2 -- 환경반응 \n",
        "        player.next_state, player.reward, player.terminated = env.step(player.action)\n",
        "        # step3 -- 경험기록 & 학습 \n",
        "        player.save_experience()\n",
        "        player.learn()\n",
        "        # step4 --종료 조건 체크 & 후속 처리\n",
        "        if env.terminated:\n",
        "            score = score + player.reward\n",
        "            scores.append(score)\n",
        "            score = 0 \n",
        "            player.state = env.reset() \n",
        "            print(f\"---에피소드{e}종료---\")\n",
        "            break\n",
        "        else: \n",
        "            score = score + player.reward\n",
        "            scores.append(score)            \n",
        "            player.state = player.next_state\n",
        "    #---에피소드끝---#\n",
        "    if scores[-1] > 0:\n",
        "        break\n",
        "```\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> 마음에 들지 않지만 꼭 외워야 하는것\n",
        ">\n",
        "> 1.  `env.step`은 항상 next_state, reward, terminated, truncated, info\n",
        ">     를 리턴한다. – 짐나지엄 라이브러리 규격때문\n",
        "> 2.  `env.reset`은 환경을 초기화할 뿐만 아니라, state, info를 반환하는\n",
        ">     기능도 있다. – 짐나지엄 라이브러리 규격때문\n",
        "> 3.  `player`는 항상 `state`와 `next_state`를 구분해서 저장한다.\n",
        ">     (다른변수들은 그렇지 않음) 이는 강화학습이\n",
        ">     MDP(마코프체인+행동+보상)구조를 따르게 때문에 생기는 고유한\n",
        ">     특징이다. – 이론적이 이유\n",
        "\n",
        "`-` 환경과 에이전트의 상호작용 이해를 위한 다이어그램:\n",
        "<https://claude.ai/public/artifacts/7fad72b5-0946-47bd-a6cd-b33b21856590>\n",
        "\n",
        "# 3. 환경의 이해\n",
        "\n",
        "## A. 데이터 축적\n",
        "\n",
        "`-` 랜덤에이전트를 이용해 무작위로 10000판을 진행해보자."
      ],
      "id": "fc39bb9f-1a97-4600-ba18-f3653c513cc5"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "player = RandomAgent()\n",
        "env = GridWorld()\n",
        "scores = [] \n",
        "score = 0 \n",
        "#\n",
        "for e in range(1,10000):\n",
        "    #---에피소드시작---#\n",
        "    while True:\n",
        "        # step1 -- 액션선택\n",
        "        player.act()\n",
        "        # step2 -- 환경반응 \n",
        "        player.next_state, player.reward, player.terminated = env.step(player.action)\n",
        "        # step3 -- 경험기록 & 학습 \n",
        "        player.save_experience()\n",
        "        player.learn()\n",
        "        # step4 --종료 조건 체크 & 후속 처리\n",
        "        if env.terminated:\n",
        "            score = score + player.reward\n",
        "            scores.append(score)\n",
        "            score = 0 \n",
        "            player.state = env.reset() \n",
        "            break\n",
        "        else: \n",
        "            score = score + player.reward\n",
        "            scores.append(score)            \n",
        "            player.state = player.next_state"
      ],
      "id": "7f808719-bb01-49d2-b064-c7d08c4347e1"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "player.n_experience"
      ],
      "id": "ef29fdf9-b027-4d88-8bd7-1e5c6d3b08d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 첫번째 `q_table`\n",
        "\n",
        "`-` 밴딧게임에서는 $Q(a)$ 를 정의했었음.\n",
        "\n",
        "-   $Q(0) = 1$\n",
        "-   $Q(1) = 10$\n",
        "\n",
        "`-` 여기에서는 $Q(s_1,s_2,a)$를 정의해야함!"
      ],
      "id": "e6e41957-420d-4a71-9f79-b36d654ae7eb"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "player.states[0], player.actions[0], player.rewards[0]"
      ],
      "id": "dc84ffd7-45da-47ae-94d7-16347398f1ee"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  }
}