{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 13wk-1: (강화학습) – 강화학습 Intro, Bandit 게임 설명, Bandit 환경\n",
        "\n",
        "설계 및 풀이\n",
        "\n",
        "최규빈  \n",
        "2025-05-28\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/13wk-1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상"
      ],
      "id": "af0d3776-ee97-47b6-aadf-bba9397ffd68"
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# {{<video https://youtu.be/playlist?list=PLQqh36zP38-zEjn2m8H8hMCHsQK8udE27&si=Sy-lnw4Kq56SRggu >}}"
      ],
      "id": "7c171ef0-8493-471d-aa90-e4861180057c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Imports"
      ],
      "id": "b68b0983-0247-4ac3-8f54-1ecdbef971ad"
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import collections"
      ],
      "id": "e67c2fbc-b914-41de-ba7c-d1ca82bbe3e0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 강화학습 Intro\n",
        "\n",
        "`-` 강화학습(대충설명): 어떠한 “(게임)환경”이 있을때 거기서 “뭘 할지”를\n",
        "학습하는 과업\n",
        "\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig1.png?raw=true\"\n",
        "alt=\"그림1: 셔튼(Sutton, Barto, et al. (1998))의 교재에서 발췌한 그림, 되게 유명한 그림이에요\" />\n",
        "<figcaption aria-hidden=\"true\">그림1: 셔튼(<span class=\"citation\"\n",
        "data-cites=\"sutton1998reinforcement\">Sutton, Barto, et al.\n",
        "(1998)</span>)의 교재에서 발췌한 그림, 되게 유명한\n",
        "그림이에요</figcaption>\n",
        "</figure>\n",
        "\n",
        "`-` 딥마인드: breakout $\\to$ 알파고\n",
        "\n",
        "-   <https://www.youtube.com/watch?v=TmPfTpjtdgg>\n",
        "\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig2.png?raw=true\"\n",
        "alt=\"그림2: 벽돌깨기\" />\n",
        "<figcaption aria-hidden=\"true\">그림2: 벽돌깨기</figcaption>\n",
        "</figure>\n",
        "\n",
        "`-` 강화학습에서 “강화”는 뭘 강화한다는것일까?\n",
        "\n",
        "-   <https://k9connoisseur.com/blogs/news/positive-reinforcement-dog-training>\n",
        "\n",
        "`-` 강화학습 미래? (이거 잘하면 먹고 살 수 있을까?)\n",
        "\n",
        "# 4. Bandit 게임 설명\n",
        "\n",
        "`-` 문제설명: 두 개의 버튼이 있다. `버튼0`을 누르면 1의 보상을,\n",
        "`버튼1`을 누르면 10의 보상을 준다고 가정\n",
        "\n",
        "-   Agent: 버튼0을 누르거나,버튼1을 누르는 존재\n",
        "-   Env: Agent의 Action을 바탕으로 Reward를 주는 존재\n",
        "\n",
        "> 주의: 이 문제 상황에서 state는 없음\n",
        "\n",
        "`-` 생성형AI로 위의 상황을 설명한것\n",
        "\n",
        "<table style=\"width:100%;\">\n",
        "<colgroup>\n",
        "<col style=\"width: 33%\" />\n",
        "<col style=\"width: 33%\" />\n",
        "<col style=\"width: 33%\" />\n",
        "</colgroup>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td style=\"text-align: left;\"><div width=\"33.3%\"\n",
        "data-layout-align=\"left\">\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gpt.png?raw=true\"\n",
        "alt=\"(a) 챗지피티로 생성한그림\" />\n",
        "<figcaption aria-hidden=\"true\">(a) 챗지피티로 생성한그림</figcaption>\n",
        "</figure>\n",
        "</div></td>\n",
        "<td style=\"text-align: left;\"><div width=\"33.3%\"\n",
        "data-layout-align=\"left\">\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gemini.png?raw=true\"\n",
        "alt=\"(b) 제미나이로 생성한 그림\" />\n",
        "<figcaption aria-hidden=\"true\">(b) 제미나이로 생성한 그림</figcaption>\n",
        "</figure>\n",
        "</div></td>\n",
        "<td style=\"text-align: left;\"><div width=\"33.3%\"\n",
        "data-layout-align=\"left\">\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-perplexity.png?raw=true\"\n",
        "alt=\"(c) 퍼플렉시티로 생성한 그림\" />\n",
        "<figcaption aria-hidden=\"true\">(c) 퍼플렉시티로 생성한 그림</figcaption>\n",
        "</figure>\n",
        "</div></td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "-   클로드로 생성:\n",
        "    <https://claude.ai/public/artifacts/1f52fcb2-ef08-4af1-8cf8-4a497d7bcc5f>\n",
        "\n",
        "`-` 게임진행양상\n",
        "\n",
        "-   처음에는 아는게 없음. 일단 “아무거나” 눌러보자. (“에이전트가\n",
        "    랜덤액션을 한다” 고 표현함 )\n",
        "-   한 20번 정도 눌러보면서 결과를 관찰함 (“에이전트가 경험을\n",
        "    축적한다”고 표현함)\n",
        "-   버튼0을 누를때는 1점, 버튼1을 누를때는 10점을 준다는 사실을 깨달음.\n",
        "    (“에이전트가 환경을 이해했다”고 표현함)\n",
        "-   버튼1을 누르는게 나한테 이득이 라는 사실을 깨달음. (“에이전트가\n",
        "    최적의 정책을 학습했다” 고 표현함)\n",
        "-   이제부터 무조건 버튼1만 누름 $\\to$ 게임 클리어 (“강화학습 성공”이라\n",
        "    표현할 수 있음)\n",
        "\n",
        "`-` 어떻게 버튼1을 누르는게 이득이라는 사실을 아는거지? $\\to$ 아래와\n",
        "같은 테이블을 만들면 된다. (`q_table`)\n",
        "\n",
        "|        |             Action0             |             Action1             |\n",
        "|:------:|:-------------------------------:|:-------------------------------:|\n",
        "| State0 | mean(Reward \\| State0, Action0) | mean(Reward \\| State0, Action1) |\n",
        "\n",
        "# 5. Bandit 환경 설계 및 풀이\n",
        "\n",
        "## A. 대충 개념만 실습"
      ],
      "id": "f9e53692-bfeb-4d80-b81b-87cc05fbe00a"
    },
    {
      "cell_type": "code",
      "execution_count": 517,
      "metadata": {},
      "outputs": [],
      "source": [
        "action_space = [0,1] \n",
        "actions_deque = collections.deque(maxlen=500)\n",
        "rewards_deque =  collections.deque(maxlen=500)\n",
        "#---#"
      ],
      "id": "3f99cc1b-e046-46d6-be53-2b2f93a4ca41"
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(10):\n",
        "    action = np.random.choice(action_space)\n",
        "    if action == 1:\n",
        "        reward = 10 \n",
        "    else:\n",
        "        reward = 1\n",
        "    actions_deque.append(action)\n",
        "    rewards_deque.append(reward)"
      ],
      "id": "13146de5-a6ab-4d02-9527-9c09084f374a"
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_deque"
      ],
      "id": "67ff5b6a-38aa-4523-ba3f-cca68585be2d"
    },
    {
      "cell_type": "code",
      "execution_count": 520,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards_deque"
      ],
      "id": "6d92dca2-4f94-4468-913f-9df41b190d5c"
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_numpy = np.array(actions_deque)\n",
        "rewards_numpy = np.array(rewards_deque)"
      ],
      "id": "8a83ec00-abff-4f00-9f91-fa2e31c7882d"
    },
    {
      "cell_type": "code",
      "execution_count": 522,
      "metadata": {},
      "outputs": [],
      "source": [
        "q0 = rewards_numpy[actions_numpy == 0].mean()\n",
        "q1 = rewards_numpy[actions_numpy == 1].mean()\n",
        "q_table = np.array([q0,q1])\n",
        "q_table"
      ],
      "id": "d1d75161-4d30-46bd-9847-f43f0c3fe61c"
    },
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {},
      "outputs": [],
      "source": [
        "action = q_table.argmax()"
      ],
      "id": "057398c0-de67-44a3-9b8c-107fdc803857"
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(5):\n",
        "    action = q_table.argmax()\n",
        "    if action == 1:\n",
        "        reward = 10 \n",
        "    else:\n",
        "        reward = 1\n",
        "    actions_deque.append(action)\n",
        "    rewards_deque.append(reward)\n",
        "    actions_numpy = np.array(actions_deque)\n",
        "    rewards_numpy = np.array(rewards_deque)    \n",
        "    q0 = rewards_numpy[actions_numpy == 0].mean()\n",
        "    q1 = rewards_numpy[actions_numpy == 1].mean()\n",
        "    q_table = np.array([q0,q1])"
      ],
      "id": "d6f136e0-be0a-4d68-bd95-2de46c107415"
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_numpy"
      ],
      "id": "9cd0dfdb-8cd1-4559-beec-35741a29d3f9"
    },
    {
      "cell_type": "code",
      "execution_count": 525,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards_numpy"
      ],
      "id": "dd121c26-6028-4a88-b167-f2292ba69786"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 클래스를 이용한 구현"
      ],
      "id": "498a9b8c-2349-4654-af37-af5fd169e0b0"
    },
    {
      "cell_type": "code",
      "execution_count": 533,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Bandit:\n",
        "    def __init__(self):\n",
        "        self.reward = None \n",
        "    def step(self,action):\n",
        "        if action == 0:\n",
        "            self.reward = 1\n",
        "        else: \n",
        "            self.reward = 10 \n",
        "        return self.reward "
      ],
      "id": "4ee96e9f-587c-4358-b728-23718a57d8c8"
    },
    {
      "cell_type": "code",
      "execution_count": 534,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = Bandit()"
      ],
      "id": "74991c83-f4ad-4dbb-8e44-4043b557e8a2"
    },
    {
      "cell_type": "code",
      "execution_count": 540,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        pass \n",
        "    def act(self):\n",
        "        # 만약에 경험이 20보다 작음 --> 랜덤액션 \n",
        "        # 경험이 20보다 크면 --> action = q_tabel.argmax()\n",
        "        pass \n",
        "    def save_experience(self):\n",
        "        # 데이터 저장 \n",
        "        pass \n",
        "    def learn(self):\n",
        "        # q_table 을 업데이트하는 과정 \n",
        "        pass"
      ],
      "id": "aa4e545e-50fc-4ede-9ba8-0ddc540e1d4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sutton, Richard S, Andrew G Barto, et al. 1998. *Reinforcement Learning:\n",
        "An Introduction*. Vol. 1. 1. MIT press Cambridge."
      ],
      "id": "ea64dd92-6029-46ff-94ed-7be5e8cee061"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  }
}