{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 13wk-1: 강화학습 (1) – Bandit\n",
        "\n",
        "최규빈  \n",
        "2024-05-28\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/13wk-1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상"
      ],
      "id": "3c1e7242-b3f8-4b38-9af2-ba3a636e85b1"
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# {{<video https://youtu.be/playlist?list=PLQqh36zP38-zEjn2m8H8hMCHsQK8udE27&si=Sy-lnw4Kq56SRggu >}}"
      ],
      "id": "7c171ef0-8493-471d-aa90-e4861180057c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Imports"
      ],
      "id": "e1d92bb8-2cf2-4560-a5ad-710c1ed03468"
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import collections"
      ],
      "id": "52b823e1-9736-49c1-a19b-5257423b91e9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 강화학습 Intro\n",
        "\n",
        "`-` 강화학습(대충설명): 어떠한 “(게임)환경”이 있을때 거기서 “뭘 할지”를\n",
        "학습하는 과업\n",
        "\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig1.png?raw=true\"\n",
        "alt=\"그림1: 셔튼(Sutton, Barto, et al. (1998))의 교재에서 발췌한 그림, 되게 유명한 그림이에요\" />\n",
        "<figcaption aria-hidden=\"true\">그림1: 셔튼(<span class=\"citation\"\n",
        "data-cites=\"sutton1998reinforcement\">Sutton, Barto, et al.\n",
        "(1998)</span>)의 교재에서 발췌한 그림, 되게 유명한\n",
        "그림이에요</figcaption>\n",
        "</figure>\n",
        "\n",
        "`-` 딥마인드: breakout $\\to$ 알파고\n",
        "\n",
        "-   <https://www.youtube.com/watch?v=TmPfTpjtdgg>\n",
        "\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig2.png?raw=true\"\n",
        "alt=\"그림2: 벽돌깨기\" />\n",
        "<figcaption aria-hidden=\"true\">그림2: 벽돌깨기</figcaption>\n",
        "</figure>\n",
        "\n",
        "`-` 강화학습에서 “강화”는 뭘 강화한다는것일까?\n",
        "\n",
        "-   <https://k9connoisseur.com/blogs/news/positive-reinforcement-dog-training>\n",
        "\n",
        "`-` 강화학습 미래? (이거 잘하면 먹고 살 수 있을까?)\n",
        "\n",
        "# 4. Bandit 게임 설명\n",
        "\n",
        "`-` 문제설명: 두 개의 버튼이 있다. `버튼0`을 누르면 1의 보상을,\n",
        "`버튼1`을 누르면 10의 보상을 준다고 가정\n",
        "\n",
        "-   Agent: 버튼0을 누르거나,버튼1을 누르는 존재\n",
        "-   Env: Agent의 Action을 바탕으로 Reward를 주는 존재\n",
        "\n",
        "> 주의: 이 문제 상황에서 state는 없음\n",
        "\n",
        "`-` 생성형AI로 위의 상황을 설명한것\n",
        "\n",
        "    <img src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gpt.png?raw=true\" style=\"width: 100%;\">\n",
        "    <p>(a) 챗지피티로 생성한 그림</p>\n",
        "\n",
        "    <img src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gemini.png?raw=true\" style=\"width: 100%;\">\n",
        "    <p>(b) 제미나이로 생성한 그림</p>\n",
        "\n",
        "    <img src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-perplexity.png?raw=true\" style=\"width: 100%;\">\n",
        "    <p>(c) 퍼플렉시티로 생성한 그림</p>\n",
        "\n",
        "-   클로드로 생성:\n",
        "    <https://claude.ai/public/artifacts/1f52fcb2-ef08-4af1-8cf8-4a497d7bcc5f>\n",
        "\n",
        "`-` 게임진행양상\n",
        "\n",
        "-   처음에는 아는게 없음. 일단 “아무거나” 눌러보자. (“에이전트가\n",
        "    랜덤액션을 한다” 고 표현함 )\n",
        "-   한 20번 정도 눌러보면서 결과를 관찰함 (“에이전트가 경험을\n",
        "    축적한다”고 표현함)\n",
        "-   버튼0을 누를때는 1점, 버튼1을 누를때는 10점을 준다는 사실을 깨달음.\n",
        "    (“에이전트가 환경을 이해했다”고 표현함)\n",
        "-   버튼1을 누르는게 나한테 이득이 라는 사실을 깨달음. (“에이전트가\n",
        "    최적의 정책을 학습했다” 고 표현함)\n",
        "-   이제부터 무조건 버튼1만 누름 $\\to$ 게임 클리어 (“강화학습 성공”이라\n",
        "    표현할 수 있음)\n",
        "\n",
        "`-` 어떻게 버튼1을 누르는게 이득이라는 사실을 아는거지? $\\to$ 아래와\n",
        "같은 테이블을 만들면 된다. (`q_table`)\n",
        "\n",
        "|        |             Action0             |             Action1             |\n",
        "|:------:|:-------------------------------:|:-------------------------------:|\n",
        "| State0 | mean(Reward \\| State0, Action0) | mean(Reward \\| State0, Action1) |\n",
        "\n",
        "# 5. Bandit 환경 설계 및 풀이\n",
        "\n",
        "## A. 대충 개념만 실습"
      ],
      "id": "64124d1d-cc36-4775-bf1b-c84bff01cb7e"
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {},
      "outputs": [],
      "source": [
        "action_space = [0,1] \n",
        "actions_deque = collections.deque(maxlen=200)\n",
        "rewards_deque = collections.deque(maxlen=200)\n",
        "#---#\n",
        "for _ in range(10):\n",
        "    action = np.random.choice(action_space)\n",
        "    if action ==0: \n",
        "        reward = 1\n",
        "    else:\n",
        "        reward = 10\n",
        "    actions_deque.append(action)\n",
        "    rewards_deque.append(reward)"
      ],
      "id": "0c101a0d-249d-4c58-836c-89a06dcf44cb"
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_deque"
      ],
      "id": "a75d37f3-44cb-4da6-b2ba-ae9d8481a9f5"
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards_deque"
      ],
      "id": "a6116391-36b0-480e-8569-e51f887bbefc"
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_numpy = np.array(actions_deque)\n",
        "rewards_numpy = np.array(rewards_deque)"
      ],
      "id": "b3e60e33-3071-448e-adf8-7fbb9208edb3"
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {},
      "outputs": [],
      "source": [
        "q0 = rewards_numpy[actions_numpy==0].mean()\n",
        "q1 = rewards_numpy[actions_numpy==1].mean()\n",
        "q_table = np.array([q0,q1])\n",
        "q_table"
      ],
      "id": "4d5cfcae-5a9f-42f1-a0c6-132c031e58b0"
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---#\n",
        "for _ in range(5):\n",
        "    #action = np.random.choice(action_space)\n",
        "    action = q_table.argmax()\n",
        "    if action ==0: \n",
        "        reward = 1\n",
        "    else:\n",
        "        reward = 10\n",
        "    actions_deque.append(action)\n",
        "    rewards_deque.append(reward)\n",
        "    actions_numpy = np.array(actions_deque)\n",
        "    rewards_numpy = np.array(rewards_deque)\n",
        "    q0 = rewards_numpy[actions_numpy==0].mean()\n",
        "    q1 = rewards_numpy[actions_numpy==1].mean()\n",
        "    q_table = np.array([q0,q1])\n",
        "    q_table"
      ],
      "id": "1fe97354-3fcc-4ea4-b624-f29e5990476a"
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GameClear"
          ]
        }
      ],
      "source": [
        "if rewards_numpy[-5:].mean() > 9:\n",
        "    print(\"GameClear\")"
      ],
      "id": "a07df6bc-913e-45af-8d02-174a98477194"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 클래스를 이용한 설계 및 풀이"
      ],
      "id": "f6ea8511-4f0b-4cd9-8985-992e2ed84bdb"
    },
    {
      "cell_type": "code",
      "execution_count": 417,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Batdit():\n",
        "    def __init__(self):\n",
        "        self.reward = None \n",
        "    def step(self,action):\n",
        "        if action == 0:\n",
        "            self.reward = 1\n",
        "        elif action == 1:\n",
        "            self.reward = 10\n",
        "        return self.reward"
      ],
      "id": "194c0fe9-f423-4716-a39c-87ccbdd5e50d"
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self):\n",
        "        self.n_experiences = 0 \n",
        "        self.action_space = [0,1]\n",
        "        self.action = None \n",
        "        self.actions_deque = collections.deque(maxlen=500)\n",
        "        self.actions_numpy = np.array(self.actions_deque)\n",
        "        self.reward = None \n",
        "        self.rewards_deque = collections.deque(maxlen=500)\n",
        "        self.rewards_numpy = np.array(self.rewards_deque)\n",
        "        self.q_table = None\n",
        "    def act(self):\n",
        "        if self.n_experiences < 20:\n",
        "            self.action = np.random.choice(self.action_space)\n",
        "        else: \n",
        "            self.action = self.q_table.argmax()\n",
        "        print(f\"버튼{self.action}누름\")\n",
        "    def save_experience(self):\n",
        "        self.n_experiences = self.n_experiences + 1\n",
        "        self.actions_deque.append(self.action)\n",
        "        self.rewards_deque.append(self.reward)\n",
        "        self.actions_numpy = np.array(self.actions_deque)\n",
        "        self.rewards_numpy = np.array(self.rewards_deque)\n",
        "    def learn(self):\n",
        "        if self.n_experiences < 20:\n",
        "            pass\n",
        "        else: \n",
        "            q0 = self.rewards_numpy[self.actions_numpy == 0].mean()\n",
        "            q1 = self.rewards_numpy[self.actions_numpy == 1].mean()\n",
        "            self.q_table = np.array([q0,q1])"
      ],
      "id": "225dbe6e-41c0-4a9e-b92f-1e253f82c1a5"
    },
    {
      "cell_type": "code",
      "execution_count": 437,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = Batdit()\n",
        "agent = Agent()"
      ],
      "id": "52a9b2d9-34b1-432d-bad1-262b95ffb99b"
    },
    {
      "cell_type": "code",
      "execution_count": 438,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "버튼0누름"
          ]
        }
      ],
      "source": [
        "agent.act()"
      ],
      "id": "8fbc3e05-de44-4bde-aaee-d78b374c37f2"
    },
    {
      "cell_type": "code",
      "execution_count": 439,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "버튼0누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼0누름\n",
            "버튼0누름\n",
            "버튼1누름\n",
            "버튼0누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼0누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼0누름\n",
            "버튼0누름\n",
            "버튼0누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "게임클리어"
          ]
        }
      ],
      "source": [
        "for _ in range(100):\n",
        "    #1. 행동\n",
        "    agent.act()\n",
        "    #2. 보상\n",
        "    agent.reward = env.step(agent.action)\n",
        "    #3. 저장 & 학습 \n",
        "    agent.save_experience()\n",
        "    agent.learn()\n",
        "    #---#\n",
        "    if (agent.n_experiences > 20) and (agent.rewards_numpy[-20:].mean() >9):\n",
        "        print(\"게임클리어\")\n",
        "        break"
      ],
      "id": "9278cfce-cf25-4d46-bcba-17b43a92bc85"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sutton, Richard S, Andrew G Barto, et al. 1998. *Reinforcement Learning:\n",
        "An Introduction*. Vol. 1. 1. MIT press Cambridge."
      ],
      "id": "dd126d49-592d-4778-9353-7ddaa4137dce"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  }
}