{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# lecture\n",
        "\n",
        "최규빈  \n",
        "2025-05-19\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/09wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상"
      ],
      "id": "006196dc-74cf-4355-89ff-c583f7b7097f"
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# {{<video https://youtu.be/playlist?list=PLQqh36zP38-zEjn2m8H8hMCHsQK8udE27&si=Sy-lnw4Kq56SRggu >}}"
      ],
      "id": "99d156c1-b7cd-4735-b27b-ed4bbe961181"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Imports"
      ],
      "id": "9b8871c0-6bf9-4ec3-a5ba-f12271cb7a00"
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import collections"
      ],
      "id": "e5af06a9-81d1-446e-baca-bdb9b5e722e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 강화학습 Intro\n",
        "\n",
        "`-` 강화학습(대충설명): 어떠한 “(게임)환경”이 있을때 거기서 “뭘 할지”를\n",
        "학습하는 과업\n",
        "\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig1.png?raw=true\"\n",
        "alt=\"그림1: 셔튼(Sutton, Barto, et al. (1998))의 교재에서 발췌한 그림, 되게 유명한 그림이에요\" />\n",
        "<figcaption aria-hidden=\"true\">그림1: 셔튼(<span class=\"citation\"\n",
        "data-cites=\"sutton1998reinforcement\">Sutton, Barto, et al.\n",
        "(1998)</span>)의 교재에서 발췌한 그림, 되게 유명한\n",
        "그림이에요</figcaption>\n",
        "</figure>\n",
        "\n",
        "`-` 딥마인드: breakout $\\to$ 알파고\n",
        "\n",
        "-   <https://www.youtube.com/watch?v=TmPfTpjtdgg>\n",
        "\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig2.png?raw=true\"\n",
        "alt=\"그림2: 벽돌깨기\" />\n",
        "<figcaption aria-hidden=\"true\">그림2: 벽돌깨기</figcaption>\n",
        "</figure>\n",
        "\n",
        "`-` 강화학습에서 “강화”는 뭘 강화한다는것일까?\n",
        "\n",
        "-   <https://k9connoisseur.com/blogs/news/positive-reinforcement-dog-training>\n",
        "\n",
        "`-` 강화학습 미래? (이거 잘하면 먹고 살 수 있을까?)\n",
        "\n",
        "# 4. Bandit 게임 설명\n",
        "\n",
        "`-` 문제설명: 두 개의 버튼이 있다. `버튼0`을 누르면 1의 보상을,\n",
        "`버튼1`을 누르면 10의 보상을 준다고 가정\n",
        "\n",
        "-   Agent: 버튼0을 누르거나,버튼1을 누르는 존재\n",
        "-   Env: Agent의 Action을 바탕으로 Reward를 주는 존재\n",
        "\n",
        "> 주의: 이 문제 상황에서 state는 없음\n",
        "\n",
        "`-` 생성형AI로 위의 상황을 설명한것\n",
        "\n",
        "    <img src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gpt.png?raw=true\" style=\"width: 100%;\">\n",
        "    <p>(a) 챗지피티로 생성한 그림</p>\n",
        "\n",
        "    <img src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gemini.png?raw=true\" style=\"width: 100%;\">\n",
        "    <p>(b) 제미나이로 생성한 그림</p>\n",
        "\n",
        "    <img src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-perplexity.png?raw=true\" style=\"width: 100%;\">\n",
        "    <p>(c) 퍼플렉시티로 생성한 그림</p>\n",
        "\n",
        "-   클로드로 생성:\n",
        "    <https://claude.ai/public/artifacts/1f52fcb2-ef08-4af1-8cf8-4a497d7bcc5f>\n",
        "\n",
        "`-` 게임진행양상\n",
        "\n",
        "-   처음에는 아는게 없음. 일단 “아무거나” 눌러보자. (“에이전트가\n",
        "    랜덤액션을 한다” 고 표현함 )\n",
        "-   한 20번 정도 눌러보면서 결과를 관찰함 (“에이전트가 경험을\n",
        "    축적한다”고 표현함)\n",
        "-   버튼0을 누를때는 1점, 버튼1을 누를때는 10점을 준다는 사실을 깨달음.\n",
        "    (“에이전트가 환경을 이해했다”고 표현함)\n",
        "-   버튼1을 누르는게 나한테 이득이 라는 사실을 깨달음. (“에이전트가\n",
        "    최적의 정책을 학습했다” 고 표현함)\n",
        "-   이제부터 무조건 버튼1만 누름 $\\to$ 게임 클리어 (“강화학습 성공”이라\n",
        "    표현할 수 있음)\n",
        "\n",
        "`-` 어떻게 버튼1을 누르는게 이득이라는 사실을 아는거지? $\\to$ 아래와\n",
        "같은 테이블을 만들면 된다. (`q_table`)\n",
        "\n",
        "|        |             Action0             |             Action1             |\n",
        "|:------:|:-------------------------------:|:-------------------------------:|\n",
        "| State0 | mean(Reward \\| State0, Action0) | mean(Reward \\| State0, Action1) |\n",
        "\n",
        "# 5. 구현\n",
        "\n",
        "## A. 대충 개념만 실습"
      ],
      "id": "4a8e4eea-457d-41d6-b294-87f973f247d5"
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {},
      "outputs": [],
      "source": [
        "action_space = [0,1] \n",
        "actions_deque = collections.deque(maxlen=200)\n",
        "rewards_deque = collections.deque(maxlen=200)\n",
        "#---#\n",
        "for _ in range(10):\n",
        "    action = np.random.choice(action_space)\n",
        "    if action ==0: \n",
        "        reward = 1\n",
        "    else:\n",
        "        reward = 10\n",
        "    actions_deque.append(action)\n",
        "    rewards_deque.append(reward)"
      ],
      "id": "c2c71723-14b2-4c3e-8907-324ba9c930f8"
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_deque"
      ],
      "id": "67719b43-0dae-4577-8f73-60d74c69716d"
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards_deque"
      ],
      "id": "d24af941-b48d-4b25-81c0-6628b7f19219"
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_numpy = np.array(actions_deque)\n",
        "rewards_numpy = np.array(rewards_deque)"
      ],
      "id": "4018124b-9bd2-4c50-ac2f-c7c44e3bcf77"
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {},
      "outputs": [],
      "source": [
        "q0 = rewards_numpy[actions_numpy==0].mean()\n",
        "q1 = rewards_numpy[actions_numpy==1].mean()\n",
        "q_table = np.array([q0,q1])\n",
        "q_table"
      ],
      "id": "9cc6c620-b481-496a-aa90-38768e6af971"
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---#\n",
        "for _ in range(5):\n",
        "    #action = np.random.choice(action_space)\n",
        "    action = q_table.argmax()\n",
        "    if action ==0: \n",
        "        reward = 1\n",
        "    else:\n",
        "        reward = 10\n",
        "    actions_deque.append(action)\n",
        "    rewards_deque.append(reward)\n",
        "    actions_numpy = np.array(actions_deque)\n",
        "    rewards_numpy = np.array(rewards_deque)\n",
        "    q0 = rewards_numpy[actions_numpy==0].mean()\n",
        "    q1 = rewards_numpy[actions_numpy==1].mean()\n",
        "    q_table = np.array([q0,q1])\n",
        "    q_table"
      ],
      "id": "09244558-e9b5-4fc3-86b1-1370da86d4bf"
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GameClear"
          ]
        }
      ],
      "source": [
        "if rewards_numpy[-5:].mean() > 9:\n",
        "    print(\"GameClear\")"
      ],
      "id": "1ece00ec-2ce2-47c3-8dc5-426f5ec74676"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 클래스를 이용한 구현"
      ],
      "id": "2142e9f7-1793-4872-a1b0-4ad3126306c6"
    },
    {
      "cell_type": "code",
      "execution_count": 417,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Batdit():\n",
        "    def __init__(self):\n",
        "        self.reward = None \n",
        "    def step(self,action):\n",
        "        if action == 0:\n",
        "            self.reward = 1\n",
        "        elif action == 1:\n",
        "            self.reward = 10\n",
        "        return self.reward"
      ],
      "id": "61984d91-1f69-457f-9b70-d4b40fe064ea"
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self):\n",
        "        self.n_experiences = 0 \n",
        "        self.action_space = [0,1]\n",
        "        self.action = None \n",
        "        self.actions_deque = collections.deque(maxlen=500)\n",
        "        self.actions_numpy = np.array(self.actions_deque)\n",
        "        self.reward = None \n",
        "        self.rewards_deque = collections.deque(maxlen=500)\n",
        "        self.rewards_numpy = np.array(self.rewards_deque)\n",
        "        self.q_table = None\n",
        "    def act(self):\n",
        "        if self.n_experiences < 20:\n",
        "            self.action = np.random.choice(self.action_space)\n",
        "        else: \n",
        "            self.action = self.q_table.argmax()\n",
        "        print(f\"버튼{self.action}누름\")\n",
        "    def save_experience(self):\n",
        "        self.n_experiences = self.n_experiences + 1\n",
        "        self.actions_deque.append(self.action)\n",
        "        self.rewards_deque.append(self.reward)\n",
        "        self.actions_numpy = np.array(self.actions_deque)\n",
        "        self.rewards_numpy = np.array(self.rewards_deque)\n",
        "    def learn(self):\n",
        "        if self.n_experiences < 20:\n",
        "            pass\n",
        "        else: \n",
        "            q0 = self.rewards_numpy[self.actions_numpy == 0].mean()\n",
        "            q1 = self.rewards_numpy[self.actions_numpy == 1].mean()\n",
        "            self.q_table = np.array([q0,q1])"
      ],
      "id": "87e52697-88bf-4f45-b3d3-ed2824475a55"
    },
    {
      "cell_type": "code",
      "execution_count": 437,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = Batdit()\n",
        "agent = Agent()"
      ],
      "id": "793b2cf2-5b46-467f-84a3-1eca97f76dbb"
    },
    {
      "cell_type": "code",
      "execution_count": 438,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "버튼0누름"
          ]
        }
      ],
      "source": [
        "agent.act()"
      ],
      "id": "50c41363-bbfa-41c0-b7a5-0affe4acd346"
    },
    {
      "cell_type": "code",
      "execution_count": 439,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "버튼0누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼0누름\n",
            "버튼0누름\n",
            "버튼1누름\n",
            "버튼0누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼0누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼0누름\n",
            "버튼0누름\n",
            "버튼0누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "버튼1누름\n",
            "게임클리어"
          ]
        }
      ],
      "source": [
        "for _ in range(100):\n",
        "    #1. 행동\n",
        "    agent.act()\n",
        "    #2. 보상\n",
        "    agent.reward = env.step(agent.action)\n",
        "    #3. 저장 & 학습 \n",
        "    agent.save_experience()\n",
        "    agent.learn()\n",
        "    #---#\n",
        "    if (agent.n_experiences > 20) and (agent.rewards_numpy[-20:].mean() >9):\n",
        "        print(\"게임클리어\")\n",
        "        break"
      ],
      "id": "37f57aef-eadd-487b-a9f6-7226909f4fee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sutton, Richard S, Andrew G Barto, et al. 1998. *Reinforcement Learning:\n",
        "An Introduction*. Vol. 1. 1. MIT press Cambridge."
      ],
      "id": "f71c787b-4f9a-43c3-950f-a2e50a32eeac"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  }
}