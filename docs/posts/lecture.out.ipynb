{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# lecture\n",
        "\n",
        "최규빈  \n",
        "2025-05-19\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/09wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상"
      ],
      "id": "c7fd7b22-d804-48cd-b5a5-9fb515db97f3"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# {{<video https://youtu.be/playlist?list=PLQqh36zP38-zEjn2m8H8hMCHsQK8udE27&si=Sy-lnw4Kq56SRggu >}}"
      ],
      "id": "89a8a3c8-183e-4b68-b40a-04d0bfca8ce1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Imports"
      ],
      "id": "dc1fbf92-da52-4b5c-8acd-1515034bf217"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "#---#\n",
        "import numpy as np\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import IPython"
      ],
      "id": "be280b13-39ac-4e0c-a67c-bbc942b53917"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Bandit 환경 설계 및 풀이\n",
        "\n",
        "## A. 대충 개념만 실습"
      ],
      "id": "a9dab12a-60cb-402b-a7c2-f34cee6c8679"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "action_space = [0,1] \n",
        "actions_deque = collections.deque(maxlen=500)\n",
        "rewards_deque =  collections.deque(maxlen=500)\n",
        "#---#"
      ],
      "id": "ffbafc7b-76fd-4e05-b2fa-f823f1f4dea3"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(10):\n",
        "    action = np.random.choice(action_space)\n",
        "    if action == 1:\n",
        "        reward = 10 \n",
        "    else:\n",
        "        reward = 1\n",
        "    actions_deque.append(action)\n",
        "    rewards_deque.append(reward)"
      ],
      "id": "ca352fd1-388f-4090-a68a-b93f139e254f"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_deque"
      ],
      "id": "df532254-b43e-4058-bd38-624e493c9507"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards_deque"
      ],
      "id": "1cf5ad96-cfbb-44a9-80dd-6f7510d9b2f9"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_numpy = np.array(actions_deque)\n",
        "rewards_numpy = np.array(rewards_deque)"
      ],
      "id": "9ad85645-23fc-49fe-80bc-10fe667421f5"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "q0 = rewards_numpy[actions_numpy == 0].mean()\n",
        "q1 = rewards_numpy[actions_numpy == 1].mean()\n",
        "q_table = np.array([q0,q1])\n",
        "q_table"
      ],
      "id": "33963107-d98d-438a-9597-8b74f89af408"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "action = q_table.argmax()"
      ],
      "id": "6662715d-e316-43b7-b281-db49f3da4b97"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(5):\n",
        "    action = q_table.argmax()\n",
        "    if action == 1:\n",
        "        reward = 10 \n",
        "    else:\n",
        "        reward = 1\n",
        "    actions_deque.append(action)\n",
        "    rewards_deque.append(reward)\n",
        "    actions_numpy = np.array(actions_deque)\n",
        "    rewards_numpy = np.array(rewards_deque)    \n",
        "    q0 = rewards_numpy[actions_numpy == 0].mean()\n",
        "    q1 = rewards_numpy[actions_numpy == 1].mean()\n",
        "    q_table = np.array([q0,q1])"
      ],
      "id": "46532b57-252e-4fdd-948e-7e328a4f2296"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_numpy"
      ],
      "id": "aa3829b3-32ba-467a-bfe8-1f3284e00bbd"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards_numpy"
      ],
      "id": "c52ba635-b54c-45e4-853a-73e3acc60b73"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 클래스를 이용한 구현"
      ],
      "id": "cc1b737c-e0fb-46e9-a7bd-a88151857fbd"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Bandit:\n",
        "    def __init__(self):\n",
        "        self.reward = None \n",
        "    def step(self,action):\n",
        "        if action == 0:\n",
        "            self.reward = 1\n",
        "        else: \n",
        "            self.reward = 10 \n",
        "        return self.reward "
      ],
      "id": "4d42cf9b-0ce1-4222-9177-3f495752779f"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        pass \n",
        "    def act(self):\n",
        "        # 만약에 경험이 20보다 작음 --> 랜덤액션 \n",
        "        # 경험이 20보다 크면 --> action = q_table.argmax()\n",
        "        pass \n",
        "    def save_experience(self):\n",
        "        # 데이터 저장 \n",
        "        pass \n",
        "    def learn(self):\n",
        "        # q_table 을 업데이트하는 과정 \n",
        "        pass"
      ],
      "id": "25a472e9-738c-4205-8ff9-e856aceaade3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------"
      ],
      "id": "0d047674-4a68-405e-80dd-31f9b89d2803"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.action = None \n",
        "        self.reward = None \n",
        "        self.actions = collections.deque(maxlen=500)\n",
        "        self.rewards = collections.deque(maxlen=500)\n",
        "        self.action_space = [0,1] \n",
        "        self.q_table = None \n",
        "        self.n_experience = 0\n",
        "    def act(self):\n",
        "        if self.n_experience < 20:\n",
        "            self.action = np.random.choice(self.action_space)\n",
        "        else: \n",
        "            self.action = self.q_table.argmax()\n",
        "        print(f\"버튼{self.action}누름!\")\n",
        "    def save_experience(self):\n",
        "        self.actions.append(self.action)\n",
        "        self.rewards.append(self.reward)\n",
        "        self.n_experience = self.n_experience + 1\n",
        "    def learn(self):\n",
        "        if self.n_experience < 20:\n",
        "            pass\n",
        "        else:\n",
        "            # q_table 을 업데이트하는 과정 \n",
        "            actions = np.array(self.actions)\n",
        "            rewards = np.array(self.rewards)\n",
        "            q0 = rewards[actions == 0].mean() # 행동0을했을때 얻는 보상의 평균값\n",
        "            q1 = rewards[actions == 1].mean()# 행동1을했을때 얻는 보상의 평균값\n",
        "            self.q_table = np.array([q0,q1])"
      ],
      "id": "a48c3d5c-ba5f-400c-8049-349d6f48d3bf"
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "버튼1누름!\n",
            "버튼0누름!\n",
            "버튼0누름!\n",
            "버튼1누름!\n",
            "버튼0누름!\n",
            "버튼0누름!\n",
            "버튼1누름!\n",
            "버튼0누름!\n",
            "버튼0누름!\n",
            "버튼0누름!\n",
            "버튼0누름!\n",
            "버튼0누름!\n",
            "버튼1누름!\n",
            "버튼0누름!\n",
            "버튼0누름!\n",
            "버튼0누름!\n",
            "버튼0누름!\n",
            "버튼0누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "버튼1누름!\n",
            "---게임클리어---"
          ]
        }
      ],
      "source": [
        "env = Bandit()\n",
        "player = Agent()\n",
        "for _ in range(100):\n",
        "    # step1: agent action \n",
        "    player.act()\n",
        "    # step2: action --> state, reward\n",
        "    player.reward = env.step(player.action)\n",
        "    # step3: agent가 데이터를 축적하고 학습\n",
        "    player.save_experience() # 데이터를 저장\n",
        "    player.learn() #저장된 데이터를 학습 \n",
        "    #---강화학습의 종료를 결정--#\n",
        "    if player.n_experience < 20:\n",
        "        pass \n",
        "    else: \n",
        "        if np.array(player.rewards)[-20:].mean() > 9.5:\n",
        "            print(\"---게임클리어---\")\n",
        "            break"
      ],
      "id": "568e511c-c955-4f80-97b2-a0ad2c264c7f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 예비학습: `gym.spaces`\n",
        "\n",
        "ref: <https://gymnasium.farama.org/>\n",
        "\n",
        "`-` 예시1"
      ],
      "id": "6f36e7d5-23e6-4cd1-8841-2707d1f72231"
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "action_space = gym.spaces.Discrete(4) \n",
        "action_space "
      ],
      "id": "2aa20310-25dd-4771-82ff-3c5efb508c96"
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "[action_space.sample() for _ in range(5)]"
      ],
      "id": "33509be6-286c-4d34-948f-7b6928c46558"
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "0 in action_space"
      ],
      "id": "1c5825aa-d6e0-4308-a23d-8a603e238afd"
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "4 in action_space"
      ],
      "id": "20347c21-9e42-4e3c-b172-e3e7a69fc12a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 예시2"
      ],
      "id": "0efd1151-7d8e-45f2-83dd-16d3dcac699a"
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "state_space"
      ],
      "id": "16b3a23b-52c7-426a-b3e2-769fe723591d"
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "[state_space.sample() for _ in range(5)]"
      ],
      "id": "d3e2dc47-e353-48fc-b61b-a93b0935215d"
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.array([0,1]) in state_space"
      ],
      "id": "5eadf302-2503-4f66-8049-795adce7f475"
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "np.array([3,3]) in state_space"
      ],
      "id": "b54b803f-5100-4ced-8d05-2b831a8c5e8a"
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "np.array([3,4]) in state_space"
      ],
      "id": "8ea21007-44be-45d3-a8c8-eb134100bcc7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. 4x4 Grid World 게임 설명\n",
        "\n",
        "## A. 게임설명\n",
        "\n",
        "`-` 문제설명: 4x4 그리드월드에서 상하좌우로 움직이는 에이전트가 목표점에\n",
        "도달하도록 하는 게임\n",
        "\n",
        "-   백문이 불여일견:\n",
        "    <https://claude.ai/public/artifacts/76e13820-2b51-4e7e-a514-00190de17c45>\n",
        "    (출처: 클로드)\n",
        "\n",
        "`-` GridWorld에서 사용되는 주요변수\n",
        "\n",
        "1.  **`State`**: 각 격자 셀이 하나의 상태이며, 에이전트는 이러한 상태 중\n",
        "    하나에 있을 수 있음.\n",
        "2.  **`Action`**: 에이전트는 현재상태에서 다음상태로 이동하기 위해\n",
        "    상,하,좌,우 중 하나의 행동을 취할 수 있음.\n",
        "3.  **`Reward`**: 에이전트가 현재상태에서 특정 action을 하면 얻어지는\n",
        "    보상.\n",
        "4.  **`Terminated`**: 하나의 에피소드가 종료되었음을 나타내는 상태.\n",
        "\n",
        "## B. 시각화"
      ],
      "id": "5d14ba84-303f-4ac2-96ee-ec134064af18"
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def show(states):\n",
        "    fig = plt.Figure()\n",
        "    ax = fig.subplots()\n",
        "    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n",
        "    sc = ax.scatter(0, 0, color='red', s=500)  \n",
        "    ax.text(0, 0, 'start', ha='center', va='center')\n",
        "    ax.text(3, 3, 'end', ha='center', va='center')\n",
        "    # Adding grid lines to the plot\n",
        "    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
        "    state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "    def update(t):\n",
        "        if states[t] in state_space:\n",
        "            s1,s2 = states[t]\n",
        "            states[t] = [s2,s1]\n",
        "            sc.set_offsets(states[t])\n",
        "        else:\n",
        "            s1,s2 = states[t]\n",
        "            s1 = s1 + 0.5 if s1 < 0 else (s1 - 0.5 if s1 > 3 else s1)\n",
        "            s2 = s2 + 0.5 if s2 < 0 else (s2 - 0.5 if s2 > 3 else s2)\n",
        "            states[t] = [s2,s1]       \n",
        "            sc.set_offsets(states[t])\n",
        "    ani = FuncAnimation(fig,update,frames=len(states))\n",
        "    display(IPython.display.HTML(ani.to_jshtml()))"
      ],
      "id": "68bfce29-e8df-4cd1-bdec-d3da2f51821b"
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "show([[0,0],[1,0],[2,0],[3,0],[4,0]]) # show 사용방법"
      ],
      "id": "af6d2912-d0bc-4a8f-978b-2f5e55b86569"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. 4x4 Grid World 환경 구현"
      ],
      "id": "e6337c38-5b1a-4a08-9ff0-01a81b5bb76f"
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.a2d = {\n",
        "            0: np.array([0,1]), # →\n",
        "            1: np.array([0,-1]), # ←  \n",
        "            2: np.array([1,0]),  # ↓\n",
        "            3: np.array([-1,0])  # ↑\n",
        "        }\n",
        "        self.state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "        self.state = np.array([0,0])\n",
        "        self.reward = None\n",
        "        self.terminated = False\n",
        "    def step(self,action):\n",
        "        self.state = self.state + self.a2d[action]\n",
        "        s1,s2 = self.state\n",
        "        if (s1==3) and (s2==3):\n",
        "            self.reward = 100 \n",
        "            self.terminated = True\n",
        "        elif self.state in self.state_space:\n",
        "            self.reward = -1 \n",
        "            self.terminated = False\n",
        "        else:\n",
        "            self.reward = -10\n",
        "            self.terminated = True\n",
        "        print(\n",
        "            f\"action = {action}\\t\"\n",
        "            f\"state = {self.state - self.a2d[action]} -> {self.state}\\t\"\n",
        "            f\"reward = {self.reward}\\t\"\n",
        "            f\"termiated = {self.terminated}\"\n",
        "        )            \n",
        "        return self.state, self.reward, self.terminated\n",
        "    def reset(self):\n",
        "        self.state = np.array([0,0])\n",
        "        self.terminated = False\n",
        "        return self.state"
      ],
      "id": "7e7f35a5-b25b-4890-89a4-493be7947533"
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = GridWorld()"
      ],
      "id": "0e143a06-fdd1-4164-8d44-bb33c037ee0e"
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 2  state = [0 0] -> [1 0]  reward = -1 termiated = False\n",
            "action = 3  state = [1 0] -> [0 0]  reward = -1 termiated = False\n",
            "action = 3  state = [0 0] -> [-1  0]    reward = -10    termiated = True"
          ]
        }
      ],
      "source": [
        "action_space = gym.spaces.Discrete(4)\n",
        "for _ in range(50):\n",
        "    action = action_space.sample()\n",
        "    env.step(action)\n",
        "    if env.terminated == True:\n",
        "        env.reset()\n",
        "        break"
      ],
      "id": "102b55d3-1c6d-48fc-b6a5-51563d902d79"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. “에이전트 $\\Leftrightarrow$ 환경” 상호작용 구현\n",
        "\n",
        "`-` 우리가 구현하고 싶은 기능\n",
        "\n",
        "-   `.act()`: 액션을 결정 –\\> 여기서는 그냥 랜덤액션\n",
        "-   `.save_experience()`: 데이터를 저장 –\\> 여기에 일단 초점을 맞추자\n",
        "-   `.learn()`: 데이터로에서 학습 –\\> 패스"
      ],
      "id": "8982a94d-747e-4855-8eef-187458177e0a"
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "    def __init__(self):\n",
        "        self.state = None \n",
        "        self.action = None \n",
        "        self.reward = None \n",
        "        self.next_state = None\n",
        "        self.terminated = None\n",
        "        #---#\n",
        "        self.states = collections.deque(maxlen=500)\n",
        "        self.actions = collections.deque(maxlen=500)\n",
        "        self.rewards = collections.deque(maxlen=500)\n",
        "        self.next_states = collections.deque(maxlen=500)\n",
        "        self.terminations = collections.deque(maxlen=500)\n",
        "        #---#\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "        self.n_experience = 0\n",
        "    def act(self):\n",
        "        self.action = self.action_space.sample()\n",
        "    def save_experience(self):\n",
        "        self.states.append(self.state)\n",
        "        self.actions.append(self.action)\n",
        "        self.rewards.append(self.reward)\n",
        "        self.next_states.append(self.next_state)\n",
        "        self.terminations.append(self.terminated)\n",
        "        self.n_experience = self.n_experience + 1\n",
        "    def learn(self):\n",
        "        pass "
      ],
      "id": "d114c3dd-66d6-4563-858f-26dc5103d4cc"
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {},
      "outputs": [],
      "source": [
        "player = RandomAgent()\n",
        "env = GridWorld()"
      ],
      "id": "4c46722e-bcfd-4384-ab1f-657cbf7f5a25"
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 2  state = [0 0] -> [1 0]  reward = -1 termiated = False\n",
            "action = 3  state = [1 0] -> [0 0]  reward = -1 termiated = False\n",
            "action = 1  state = [0 0] -> [ 0 -1]    reward = -10    termiated = True"
          ]
        }
      ],
      "source": [
        "for t in range(50):\n",
        "    # step1 -- 에이전트가 action을 함 \n",
        "    player.act()\n",
        "    # step2 -- 환경이 에이전트의 action을 보고 next_state, reward, terminated \n",
        "    player.next_state, player.reward, player.terminated = env.step(player.action)\n",
        "    # step3 -- 에이전트가 save & learn\n",
        "    player.save_experience()\n",
        "    player.learn()\n",
        "    # step4 -- next iteration \n",
        "    player.state = player.next_state\n",
        "    if env.terminated:\n",
        "        player.state = env.reset()\n",
        "        break"
      ],
      "id": "2a6f31f2-d61c-4c31-9c7c-0591de2dccf8"
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 1  state = [0 0] -> [ 0 -1]    reward = -10    termiated = True\n",
            "---에피소드1종료---\n",
            "action = 3  state = [0 0] -> [-1  0]    reward = -10    termiated = True\n",
            "---에피소드2종료---\n",
            "action = 1  state = [0 0] -> [ 0 -1]    reward = -10    termiated = True\n",
            "---에피소드3종료---\n",
            "action = 1  state = [0 0] -> [ 0 -1]    reward = -10    termiated = True\n",
            "---에피소드4종료---\n",
            "action = 0  state = [0 0] -> [0 1]  reward = -1 termiated = False\n",
            "action = 1  state = [0 1] -> [0 0]  reward = -1 termiated = False\n",
            "action = 3  state = [0 0] -> [-1  0]    reward = -10    termiated = True\n",
            "---에피소드5종료---\n",
            "action = 0  state = [0 0] -> [0 1]  reward = -1 termiated = False\n",
            "action = 0  state = [0 1] -> [0 2]  reward = -1 termiated = False\n",
            "action = 2  state = [0 2] -> [1 2]  reward = -1 termiated = False\n",
            "action = 1  state = [1 2] -> [1 1]  reward = -1 termiated = False\n",
            "action = 1  state = [1 1] -> [1 0]  reward = -1 termiated = False\n",
            "action = 3  state = [1 0] -> [0 0]  reward = -1 termiated = False\n",
            "action = 3  state = [0 0] -> [-1  0]    reward = -10    termiated = True\n",
            "---에피소드6종료---\n",
            "action = 3  state = [0 0] -> [-1  0]    reward = -10    termiated = True\n",
            "---에피소드7종료---\n",
            "action = 3  state = [0 0] -> [-1  0]    reward = -10    termiated = True\n",
            "---에피소드8종료---\n",
            "action = 2  state = [0 0] -> [1 0]  reward = -1 termiated = False\n",
            "action = 1  state = [1 0] -> [ 1 -1]    reward = -10    termiated = True\n",
            "---에피소드9종료---\n",
            "action = 2  state = [0 0] -> [1 0]  reward = -1 termiated = False\n",
            "action = 0  state = [1 0] -> [1 1]  reward = -1 termiated = False\n",
            "action = 2  state = [1 1] -> [2 1]  reward = -1 termiated = False\n",
            "action = 0  state = [2 1] -> [2 2]  reward = -1 termiated = False\n",
            "action = 0  state = [2 2] -> [2 3]  reward = -1 termiated = False\n",
            "action = 2  state = [2 3] -> [3 3]  reward = 100    termiated = True\n",
            "---에피소드10종료---"
          ]
        }
      ],
      "source": [
        "scores = [] \n",
        "for e in range(1,20):\n",
        "    score = 0\n",
        "    player.state = env.reset()\n",
        "    for t in range(50):\n",
        "        # step1 -- 에이전트가 action을 함 \n",
        "        player.act()\n",
        "        # step2 -- 환경이 에이전트의 action을 보고 next_state, reward, terminated \n",
        "        player.next_state, player.reward, player.terminated = env.step(player.action)\n",
        "        # step3 -- 에이전트가 save & learn\n",
        "        player.save_experience()\n",
        "        player.learn()\n",
        "        # step4 -- next iteration \n",
        "        player.state = player.next_state\n",
        "        score = score + player.reward\n",
        "        if env.terminated:\n",
        "            scores.append(score)\n",
        "            print(f\"---에피소드{e}종료---\")\n",
        "            break\n",
        "    if scores[-1] > 0:\n",
        "        break"
      ],
      "id": "3029a59f-ed4d-41ae-b5f6-99f3fbb992a9"
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {},
      "outputs": [],
      "source": [
        "paths = [np.array([0,0])]+ list(player.next_states)[-6:]\n",
        "show(paths)"
      ],
      "id": "df51b7f6-5bc1-4d4b-9efd-5b88a8972258"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  }
}