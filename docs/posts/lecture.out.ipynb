{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# lecture\n",
        "\n",
        "최규빈  \n",
        "2025-05-19\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/09wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상"
      ],
      "id": "69e17b44-202d-4777-add5-8582b38a7e1f"
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# {{<video https://youtu.be/playlist?list=PLQqh36zP38-zEjn2m8H8hMCHsQK8udE27&si=Sy-lnw4Kq56SRggu >}}"
      ],
      "id": "99d156c1-b7cd-4735-b27b-ed4bbe961181"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Imports"
      ],
      "id": "a5a22614-7596-454e-aaf4-323ecfeb78aa"
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import collections"
      ],
      "id": "e5af06a9-81d1-446e-baca-bdb9b5e722e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 강화학습 Intro\n",
        "\n",
        "`-` 강화학습(대충설명): 어떠한 “(게임)환경”이 있을때 거기서 “뭘 할지”를\n",
        "학습하는 과업\n",
        "\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig1.png?raw=true\"\n",
        "alt=\"그림1: 셔튼(Sutton, Barto, et al. (1998))의 교재에서 발췌한 그림, 되게 유명한 그림이에요\" />\n",
        "<figcaption aria-hidden=\"true\">그림1: 셔튼(<span class=\"citation\"\n",
        "data-cites=\"sutton1998reinforcement\">Sutton, Barto, et al.\n",
        "(1998)</span>)의 교재에서 발췌한 그림, 되게 유명한\n",
        "그림이에요</figcaption>\n",
        "</figure>\n",
        "\n",
        "`-` 딥마인드: breakout $\\to$ 알파고\n",
        "\n",
        "-   <https://www.youtube.com/watch?v=TmPfTpjtdgg>\n",
        "\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig2.png?raw=true\"\n",
        "alt=\"그림2: 벽돌깨기\" />\n",
        "<figcaption aria-hidden=\"true\">그림2: 벽돌깨기</figcaption>\n",
        "</figure>\n",
        "\n",
        "`-` 강화학습에서 “강화”는 뭘 강화한다는것일까?\n",
        "\n",
        "-   <https://k9connoisseur.com/blogs/news/positive-reinforcement-dog-training>\n",
        "\n",
        "`-` 강화학습 미래? (이거 잘하면 먹고 살 수 있을까?)\n",
        "\n",
        "# 4. Bandit 게임 설명\n",
        "\n",
        "`-` 문제설명: 두 개의 버튼이 있다. `버튼0`을 누르면 1의 보상을,\n",
        "`버튼1`을 누르면 10의 보상을 준다고 가정\n",
        "\n",
        "-   Agent: 버튼0을 누르거나,버튼1을 누르는 존재\n",
        "-   Env: Agent의 Action을 바탕으로 Reward를 주는 존재\n",
        "\n",
        "> 주의: 이 문제 상황에서 state는 없음\n",
        "\n",
        "`-` 생성형AI로 위의 상황을 설명한것\n",
        "\n",
        "<table style=\"width:100%;\">\n",
        "<colgroup>\n",
        "<col style=\"width: 33%\" />\n",
        "<col style=\"width: 33%\" />\n",
        "<col style=\"width: 33%\" />\n",
        "</colgroup>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td style=\"text-align: left;\"><div width=\"33.3%\"\n",
        "data-layout-align=\"left\">\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gpt.png?raw=true\"\n",
        "alt=\"(a) 챗지피티로 생성한그림\" />\n",
        "<figcaption aria-hidden=\"true\">(a) 챗지피티로 생성한그림</figcaption>\n",
        "</figure>\n",
        "</div></td>\n",
        "<td style=\"text-align: left;\"><div width=\"33.3%\"\n",
        "data-layout-align=\"left\">\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gemini.png?raw=true\"\n",
        "alt=\"(b) 제미나이로 생성한 그림\" />\n",
        "<figcaption aria-hidden=\"true\">(b) 제미나이로 생성한 그림</figcaption>\n",
        "</figure>\n",
        "</div></td>\n",
        "<td style=\"text-align: left;\"><div width=\"33.3%\"\n",
        "data-layout-align=\"left\">\n",
        "<figure class=\"margin-caption\">\n",
        "<img\n",
        "src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-perplexity.png?raw=true\"\n",
        "alt=\"(c) 퍼플렉시티로 생성한 그림\" />\n",
        "<figcaption aria-hidden=\"true\">(c) 퍼플렉시티로 생성한 그림</figcaption>\n",
        "</figure>\n",
        "</div></td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "-   클로드로 생성:\n",
        "    <https://claude.ai/public/artifacts/1f52fcb2-ef08-4af1-8cf8-4a497d7bcc5f>\n",
        "\n",
        "`-` 게임진행양상\n",
        "\n",
        "-   처음에는 아는게 없음. 일단 “아무거나” 눌러보자. (“에이전트가\n",
        "    랜덤액션을 한다” 고 표현함 )\n",
        "-   한 20번 정도 눌러보면서 결과를 관찰함 (“에이전트가 경험을\n",
        "    축적한다”고 표현함)\n",
        "-   버튼0을 누를때는 1점, 버튼1을 누를때는 10점을 준다는 사실을 깨달음.\n",
        "    (“에이전트가 환경을 이해했다”고 표현함)\n",
        "-   버튼1을 누르는게 나한테 이득이 라는 사실을 깨달음. (“에이전트가\n",
        "    최적의 정책을 학습했다” 고 표현함)\n",
        "-   이제부터 무조건 버튼1만 누름 $\\to$ 게임 클리어 (“강화학습 성공”이라\n",
        "    표현할 수 있음)\n",
        "\n",
        "`-` 어떻게 버튼1을 누르는게 이득이라는 사실을 아는거지? $\\to$ 아래와\n",
        "같은 테이블을 만들면 된다. (`q_table`)\n",
        "\n",
        "|        |             Action0             |             Action1             |\n",
        "|:------:|:-------------------------------:|:-------------------------------:|\n",
        "| State0 | mean(Reward \\| State0, Action0) | mean(Reward \\| State0, Action1) |\n",
        "\n",
        "# 5. 구현\n",
        "\n",
        "## A. 대충 개념만 실습"
      ],
      "id": "058819f4-9c6d-497f-a57e-f5cf75272eb9"
    },
    {
      "cell_type": "code",
      "execution_count": 517,
      "metadata": {},
      "outputs": [],
      "source": [
        "action_space = [0,1] \n",
        "actions_deque = collections.deque(maxlen=500)\n",
        "rewards_deque =  collections.deque(maxlen=500)\n",
        "#---#"
      ],
      "id": "e11541cc-a81d-437f-94b6-5cfd1f6b7b54"
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(10):\n",
        "    action = np.random.choice(action_space)\n",
        "    if action == 1:\n",
        "        reward = 10 \n",
        "    else:\n",
        "        reward = 1\n",
        "    actions_deque.append(action)\n",
        "    rewards_deque.append(reward)"
      ],
      "id": "6881c709-797e-474e-a0e2-f285060118ae"
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_deque"
      ],
      "id": "c07fb7f3-adeb-43f7-881f-1734ee3f3228"
    },
    {
      "cell_type": "code",
      "execution_count": 520,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards_deque"
      ],
      "id": "24dd511c-51d7-42dc-8793-9e9f8135fed0"
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_numpy = np.array(actions_deque)\n",
        "rewards_numpy = np.array(rewards_deque)"
      ],
      "id": "7fd97f5a-e5b0-4c0a-b301-86a89670ccab"
    },
    {
      "cell_type": "code",
      "execution_count": 522,
      "metadata": {},
      "outputs": [],
      "source": [
        "q0 = rewards_numpy[actions_numpy == 0].mean()\n",
        "q1 = rewards_numpy[actions_numpy == 1].mean()\n",
        "q_table = np.array([q0,q1])\n",
        "q_table"
      ],
      "id": "eb4ef9c6-3218-46bd-831e-bc414b320c16"
    },
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {},
      "outputs": [],
      "source": [
        "action = q_table.argmax()"
      ],
      "id": "d69e0248-6f60-4fbf-adda-54529428166d"
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(5):\n",
        "    action = q_table.argmax()\n",
        "    if action == 1:\n",
        "        reward = 10 \n",
        "    else:\n",
        "        reward = 1\n",
        "    actions_deque.append(action)\n",
        "    rewards_deque.append(reward)\n",
        "    actions_numpy = np.array(actions_deque)\n",
        "    rewards_numpy = np.array(rewards_deque)    \n",
        "    q0 = rewards_numpy[actions_numpy == 0].mean()\n",
        "    q1 = rewards_numpy[actions_numpy == 1].mean()\n",
        "    q_table = np.array([q0,q1])"
      ],
      "id": "8c4a5aa4-a7de-494d-8a53-b0c765f1f3d8"
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_numpy"
      ],
      "id": "e4d02dd3-15b4-4b43-9a7c-81c89ec899a5"
    },
    {
      "cell_type": "code",
      "execution_count": 525,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards_numpy"
      ],
      "id": "da08ea5a-c0ad-4a74-af5e-3f42314ede05"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 클래스를 이용한 구현"
      ],
      "id": "d1bf6003-702a-4776-a7d6-152e203d6725"
    },
    {
      "cell_type": "code",
      "execution_count": 533,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Bandit:\n",
        "    def __init__(self):\n",
        "        self.reward = None \n",
        "    def step(self,action):\n",
        "        if action == 0:\n",
        "            self.reward = 1\n",
        "        else: \n",
        "            self.reward = 10 \n",
        "        return self.reward "
      ],
      "id": "95e30c5e-5415-4820-8e70-f193c98a2a7f"
    },
    {
      "cell_type": "code",
      "execution_count": 534,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = Bandit()"
      ],
      "id": "6a3275de-bc5c-490c-8675-7fa0e20e68ae"
    },
    {
      "cell_type": "code",
      "execution_count": 540,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        pass \n",
        "    def act(self):\n",
        "        # 만약에 경험이 20보다 작음 --> 랜덤액션 \n",
        "        # 경험이 20보다 크면 --> action = q_tabel.argmax()\n",
        "        pass \n",
        "    def save_experience(self):\n",
        "        # 데이터 저장 \n",
        "        pass \n",
        "    def learn(self):\n",
        "        # q_table 을 업데이트하는 과정 \n",
        "        pass"
      ],
      "id": "0c914273-8f9c-4295-8079-b4e0ce4aab8a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sutton, Richard S, Andrew G Barto, et al. 1998. *Reinforcement Learning:\n",
        "An Introduction*. Vol. 1. 1. MIT press Cambridge."
      ],
      "id": "30b2e9f5-d2e4-44bf-a28d-8f57d425f9dd"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  }
}