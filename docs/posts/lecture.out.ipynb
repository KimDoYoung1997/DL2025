{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# lecture\n",
        "\n",
        "최규빈  \n",
        "2025-05-19\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/09wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상"
      ],
      "id": "ba05aaa6-90da-4b3a-8040-6d0e78d87225"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "#{{<video https://youtu.be/playlist?list=PLQqh36zP38-yfHwkWM_aHLQKTfKtoA7Oy&si=sgzwOEtflO1POup0 >}}"
      ],
      "id": "c312ab0b-05da-4723-b350-83e344abc5e9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Imports"
      ],
      "id": "3fa1692e-dbf2-4047-841f-858b4268c452"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "1faf4c40-64d2-4fd5-b89f-e08f6c0aa6bf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Data – `AbAcAd`\n",
        "\n",
        "`-` 데이터정리"
      ],
      "id": "63ca8094-d0cc-4d21-87cb-c669a64d879a"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "txt = list('AbAcAd'*50)\n",
        "txt[:10]"
      ],
      "id": "5b481b28-ac17-404e-97a6-2791382d8a99"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "df_train = pd.DataFrame({'x':txt[:-1], 'y':txt[1:]})\n",
        "df_train[:5]"
      ],
      "id": "5f56d4e2-3a1a-4d02-8940-6f5e43d52ee4"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "x = torch.tensor(df_train.x.map({'A':0,'b':1,'c':2,'d':3}))\n",
        "y = torch.tensor(df_train.y.map({'A':0,'b':1,'c':2,'d':3}))\n",
        "X = torch.nn.functional.one_hot(x).float()\n",
        "y = torch.nn.functional.one_hot(y).float()"
      ],
      "id": "ba1915dd-4936-4501-8261-f9a662d52352"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.shape, y.shape"
      ],
      "id": "2e72bc52-1188-4177-8def-6616a819db22"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. `rNNCell` – 복습\n",
        "\n",
        "# 5. `torch.nn.RNNCell`\n",
        "\n",
        "ref: <https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html>\n",
        "\n",
        "`-` `torch.nn.RNNCell`을 이용하여 학습\n",
        "\n",
        "`-` `torch.nn.RNNCell`의 가중치를 이전에 직접 설계한 `rNNCell`와\n",
        "동일하게 설정한 이후 학습\n",
        "\n",
        "-   왜 이런것을 하지? 우리가 직접만들어본 클래스 `rNNCell`이 torch에서\n",
        "    기본제공하는 `torch.nn.RNNCell`와 동일기능을 수행한다는 것을\n",
        "    확인하기 위함\n",
        "\n",
        "# 6. `torch.nn.RNN`\n",
        "\n",
        "ref: <https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html>\n",
        "\n",
        "|     | `RNNCell`, 배치사용 X | `RNNCell`, 배치사용 O |\n",
        "|:---:|:---------------------:|:---------------------:|\n",
        "|  X  |     $(L,H_{in})$      |   $(L, N, H_{in})$    |\n",
        "|  h  |     $(L,H_{out})$     |   $(L, N, H_{out})$   |\n",
        "|  y  |        $(L,Q)$        |      $(L, N, Q)$      |\n",
        "| Xt  |      $(H_{in},)$      |     $(N, H_{in})$     |\n",
        "| ht  |     $(H_{out},)$      |    $(N, H_{out})$     |\n",
        "| yt  |        $(Q,)$         |        $(N,Q)$        |\n",
        "\n",
        "|     |           `RNN`, 배치사용 X           |            `RNN`, 배치사용 O            |\n",
        "|:----------------------:|:----------------------:|:----------------------:|\n",
        "|  X  |             $(L,H_{in})$              |            $(L, N, H_{in})$             |\n",
        "|  h  |             $(L,H_{out})$             |            $(L, N, H_{out})$            |\n",
        "|  y  |                $(L,Q)$                |               $(L, N, Q)$               |\n",
        "| hx  | $(D\\times {\\tt num\\_layers},H_{out})$ | $(D\\times {\\tt num\\_layers},N,H_{out})$ |\n",
        "\n",
        "`-` `torch.nn.RNN`을 활용한 학습\n",
        "\n",
        "`-` `torch.nn.RNN`의 가중치를 이전에 직접 설계한 `rNNCell`와 동일하게\n",
        "설정한 이후 학습\n",
        "\n",
        "# 7. `torch.nn.LSTM`\n",
        "\n",
        "`-` `torch.nn.LSTM`을 이용하여 학습\n",
        "\n",
        "# A1. 자잘한 용어 정리 ($\\star$)\n",
        "\n",
        "## A. ${\\bf X}$, ${\\bf y}$\n",
        "\n",
        "`-` X, y를 지칭하는 이름\n",
        "\n",
        "| 구분 | 용어                            | 설명                                                                                               |\n",
        "|----------|-----------------------------------------------------|----------|\n",
        "| X    | 설명변수                        | 종속변수(반응변수)를 설명하거나 예측하는 데 사용되는 변수로, 전통 통계 및 머신러닝에서의 입력 역할 |\n",
        "|      | 독립변수 (Independent Variable) | 전통적인 통계학 및 회귀 분석 문맥에서 사용됨                                                       |\n",
        "|      | 입력변수 (Input Variable)       | 머신러닝 모델에서 입력 데이터로 사용되며, 특히 신경망 구조 등에서 많이 쓰임                        |\n",
        "|      | 특징 / 특성 (Feature)           | 머신러닝, 데이터마이닝, 딥러닝 등에서 데이터를 구성하는 속성 또는 설명 변수로 사용됨               |\n",
        "|      | 예측 변수 (Predictor)           | 예측 모델 설계 시 독립변수를 지칭하는 용어로, 모델링/통계 분석 문맥에서 흔히 사용됨                |\n",
        "|      | 공변량 (Covariate)              | 실험 디자인, 특히 임상연구나 사회과학 연구에서 제어 변수로 사용됨                                  |\n",
        "| y    | 반응변수                        | 독립변수의 영향을 받는 결과 변수로, 모델링이나 인과 추론에서 핵심적인 대상                         |\n",
        "|      | 종속변수 (Dependent Variable)   | 전통 통계학과 회귀분석에서 사용되며, 독립변수의 영향을 받는 변수로 정의됨                          |\n",
        "|      | 출력변수 (Output Variable)      | 머신러닝 및 딥러닝에서 모델의 예측 결과로 출력되는 값으로 사용됨                                   |\n",
        "|      | 타겟 / 정답 (Target / Label)    | 지도학습에서 모델이 학습해야 하는 실제 정답값을 의미하며, 분류/회귀 문제에 공통적으로 사용됨       |\n",
        "\n",
        "`-` 더 다양함: <https://ko.wikipedia.org/wiki/독립변수와_종속변수>\n",
        "\n",
        "## B. 지도학습\n",
        "\n",
        "`-` 우리가 수업에서 다루는 데이터는 주로 아래와 같은 느낌이다.\n",
        "\n",
        "1.  데이터는 $(X,y)$의 형태로 정리되어 있다.\n",
        "\n",
        "2.  $y$는 우리가 관심이 있는 변수이다. 즉 우리는 $y$를 적절하게 추정하는\n",
        "    것에 관심이 있다.\n",
        "\n",
        "3.  $X$는 $y$를 추정하기 위해 필요한 정보이다.\n",
        "\n",
        "|           $X$           |           $y$            |        비고        |      순서      |               예시               |\n",
        "|:------------:|:--------------:|:----------:|:----------:|:------------------:|\n",
        "|       기온(온도)        | 아이스 아메리카노 판매량 |        회귀        |    상관없음    | 날씨가 판매량에 미치는 영향 분석 |\n",
        "|          스펙           |        합격 여부         |      로지스틱      |    상관없음    |     입사 지원자의 합격 예측      |\n",
        "|         이미지          |         카테고리         | 합성곱신경망 (CNN) |    상관없음    |      개/고양이 이미지 구분       |\n",
        "|    유저, 아이템 정보    |           평점           |     추천시스템     |    상관없음    |        넷플릭스 영화 추천        |\n",
        "| 처음 $m$개의 단어(문장) |  이후 1개의 단어(문장)   |  순환신경망 (RNN)  | 순서 상관 있음 |   챗봇, 문장 생성, 언어 모델링   |\n",
        "| 처음 $m$개의 단어(문장) |         카테고리         |  순환신경망 (RNN)  | 순서 상관 있음 |        영화리뷰 감정 분류        |\n",
        "\n",
        "`-` 이러한 문제상황, 즉 $(X,y)$가 주어졌을때 $X \\to y$를 추정하는 문제를\n",
        "supervised learning 이라한다.\n",
        "\n",
        "## C. 모델이란?\n",
        "\n",
        "`-` 통계학에서 모델은 y와 x의 관계를 의미하며 오차항의 설계를 포함하는\n",
        "개념이다. 이는 통계학이 “데이터 = 정보 + 오차”의 관점을 유지하기\n",
        "때문이다. 따라서 통계학에서 모델링이란\n",
        "\n",
        "$$y_i = net(x_i) + \\epsilon_i$$\n",
        "\n",
        "에서 (1) 적절한 함수 $net$를 선택하는 일 (2) 적절한 오차항 $\\epsilon_i$\n",
        "을 설계하는일 모두를 포함한다.\n",
        "\n",
        "`-` 딥러닝 혹은 머신러닝에서 모델은 단순히\n",
        "\n",
        "$$y_i \\approx net(x_i)$$\n",
        "\n",
        "를 의미하는 경우가 많다. 즉 “model=net”라고 생각해도 무방하다. 이 경우\n",
        "“모델링”이란 단순히 적절한 $net$을 설계하는 것만을 의미할 경우가 많다.\n",
        "\n",
        "`-` 그래서 생긴일\n",
        "\n",
        "-   통계학교재 특징: 분류문제와 회귀문제를 엄밀하게 구분하지 않는다.\n",
        "    사실 오차항만 다를뿐이지 크게보면 같은 회귀모형이라는 관점이다.\n",
        "    그래서 일반화선형모형(GLM)이라는 용어를 쓴다.\n",
        "-   머신러닝/딥러닝교재 특징: 회귀문제와 분류문제를 구분해서 설명한다.\n",
        "    (표도 만듦) 이는 오차항에 대한 기술을 모호하게 하여 생기는 현상이다.\n",
        "\n",
        "## D. 학습이란?\n",
        "\n",
        "`-` 학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한\n",
        "“규칙” 혹은 “원리”를 찾는 것이다.\n",
        "\n",
        "-   학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한\n",
        "    “맵핑”을 찾는 것이다.\n",
        "-   학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한\n",
        "    “함수”을 찾는 것이다. 즉 $y\\approx f(X)$가 되도록 만드는 $f$를 잘\n",
        "    찾는 것이다. (이 경우 “함수를 추정한다”라고 표현)\n",
        "-   학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한\n",
        "    “모델” 혹은 “모형”을 찾는 것이다. 즉 $y\\approx model(X)$가 되도록\n",
        "    만드는 $model$을 잘 찾는 것이다. (이 경우 “모형을 학습시킨다”라고\n",
        "    표현)\n",
        "-   **학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는\n",
        "    어떠한 “네트워크”을 찾는 것이다. 즉 $y\\approx net(X)$가 되도록\n",
        "    만드는 $net$을 잘 찾는 것이다. (이 경우 “네트워크를 학습시킨다”라고\n",
        "    표현)**\n",
        "\n",
        "`-` prediction이란 학습과정에서 찾은 “규칙” 혹은 “원리”를 $X$에 적용하여\n",
        "$\\hat{y}$을 구하는 과정이다. 학습과정에서 찾은 규칙 혹은 원리는\n",
        "$f$,$model$,$net$ 으로 생각가능한데 이에 따르면 아래가 성립한다.\n",
        "\n",
        "-   $\\hat{y} = f(X)$\n",
        "-   $\\hat{y} = model(X)$\n",
        "-   $\\hat{y} = net(X)$\n",
        "\n",
        "## E. $\\hat{y}$를 부르는 다양한 이름\n",
        "\n",
        "`-` $\\hat{y}$는 $X$가 주어진 자료에 있는 값인지 아니면 새로운 값 인지에\n",
        "따라 지칭하는 이름이 미묘하게 다르다.\n",
        "\n",
        "1.  $X \\in data$: $\\hat{y}=net(X)$ 는 predicted value, fitted value 라고\n",
        "    부른다.\n",
        "\n",
        "2.  $X \\notin data$: $\\hat{y}=net(X)$ 는 predicted value, predicted\n",
        "    value with new data 라고 부른다.\n",
        "\n",
        "## F. 다양한 코드들\n",
        "\n",
        "`-` 파이썬 코드..\n",
        "\n",
        "``` python\n",
        "#Python\n",
        "predictor.fit(X,y) # autogluon 에서 \"학습\"을 의미하는 과정\n",
        "model.fit(X,y) # sklearn 에서 \"학습\"을 의미하는 과정\n",
        "trainer.train() # huggingface 에서 \"학습\"을 의미하는 과정\n",
        "trainer.predict(dataset) # huggingface 에서 \"예측\"을 의미하는 과정\n",
        "model.fit(x, y, batch_size=32, epochs=10) # keras에서 \"학습\"을 의미하는 과정\n",
        "model.predict(test_img) # keras에서 \"예측\"을 의미하는 과정 \n",
        "```\n",
        "\n",
        "`-` R 코드..\n",
        "\n",
        "``` r\n",
        "# R\n",
        "ols <- lm(y~x) # 선형회귀분석에서 학습을 의미하는 함수\n",
        "ols$fitted.values # 선형회귀분석에서 yhat을 출력 \n",
        "predict(ols, newdata=test) # 선형회귀분석에서 test에 대한 예측값을 출력하는 함수\n",
        "ols$coef # 선형회귀분석에서 weight를 확인하는 방법\n",
        "```\n",
        "\n",
        "# A2. 신경망관련 용어\n",
        "\n",
        "`-` 은근히 용어가 헷갈리는데, 뜻을 좀 살펴보자.\n",
        "\n",
        "-   ANN: 인공신경망\n",
        "-   MLP: 다층퍼셉트론 (레이어가 여러개 있어요)\n",
        "-   DNN: 깊은신경망, 심층신경망\n",
        "-   CNN: 합성곱신경망\n",
        "-   RNN: 순환신경망\n",
        "\n",
        "`# 예시1` – MLP, DNN\n",
        "\n",
        "``` python\n",
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=1,out_features=2),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features=2,out_features=2),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features=2,out_features=1),    \n",
        "    torch.nn.Sigmoid()\n",
        ")\n",
        "```\n",
        "\n",
        "-   ANN: O\n",
        "-   MLP: O\n",
        "-   DNN: O\n",
        "-   CNN: X (합성곱레이어가 없으므로)\n",
        "-   RNN: X (순환구조가 없으므로)\n",
        "\n",
        "`#`\n",
        "\n",
        "`# 예시2` – MLP, Shallow Network\n",
        "\n",
        "``` python\n",
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=1,out_features=2),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features=2,out_features=1),\n",
        "    torch.nn.Sigmoid()\n",
        ")\n",
        "```\n",
        "\n",
        "-   ANN: O\n",
        "-   MLP: O\n",
        "-   DNN: X (깊은 신경망으로 생각하려면 더 많은 레이어가 필요함. 합의된\n",
        "    기준은 히든레이어 2장이상, 이걸 설명하기 위해서 얕은 신경망이란\n",
        "    용어도 씀)\n",
        "-   CNN: X (합성곱레이어가 없으므로)\n",
        "-   RNN: X (순환구조가 없으므로)\n",
        "\n",
        "`#`\n",
        "\n",
        "`# 예시3` – MLP, DNN, Wide NN\n",
        "\n",
        "``` python\n",
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=1,out_features=1048576),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features=1048576,out_features=1048576),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features=1048576,out_features=1),\n",
        "    torch.nn.Sigmoid(),    \n",
        ")\n",
        "```\n",
        "\n",
        "-   ANN: O\n",
        "-   MLP: O\n",
        "-   DNN: O (깊긴한데 이정도면 모양이 깊다기 보다는 넓은 신경망임, 그래서\n",
        "    어떤 연구에서는 이걸 넓은 신경망이라 부르기도 함)\n",
        "-   CNN: X (합성곱레이어가 없으므로)\n",
        "-   RNN: X (순환구조가 없으므로)\n",
        "\n",
        "`# 예시4` – CNN\n",
        "\n",
        "``` python\n",
        "net = torch.nn.Sequential(\n",
        "    # Layer1\n",
        "    torch.nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    torch.nn.LeakyReLU(0.2),\n",
        "    # Layer2\n",
        "    torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    torch.nn.BatchNorm2d(128),\n",
        "    torch.nn.LeakyReLU(0.2),\n",
        "    # Layer3\n",
        "    torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    torch.nn.BatchNorm2d(256),\n",
        "    torch.nn.LeakyReLU(0.2),\n",
        "    # Layer4\n",
        "    torch.nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    torch.nn.BatchNorm2d(512),\n",
        "    torch.nn.LeakyReLU(0.2),\n",
        "    # Layer5\n",
        "    torch.nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "    torch.nn.Sigmoid(),\n",
        "    torch.nn.Flatten()\n",
        ")\n",
        "```\n",
        "\n",
        "-   ANN: O\n",
        "-   MLP: X (합성곱연결이 포함되어있으므로, MLP가 아님, 완전연결만\n",
        "    포함해야 MLP임)  \n",
        "-   DNN: O\n",
        "-   CNN: O (합성곱레이어를 포함하고 있으므로)\n",
        "-   RNN: X (순환구조가 없으므로)\n",
        "\n",
        "`#`\n",
        "\n",
        "`# 예시5` – CNN\n",
        "\n",
        "``` python\n",
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Conv2d(1,16,(5,5)),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.MaxPool2d((2,2)),\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(2304,1),\n",
        "    torch.nn.Sigmoid()\n",
        ")\n",
        "```\n",
        "\n",
        "-   ANN: O\n",
        "-   MLP: X\n",
        "-   DNN: X? (히든레이어가 1장이므로..)\n",
        "-   CNN: O (합성곱레이어를 포함하고 있으므로)\n",
        "-   RNN: X (순환구조가 없으므로)\n",
        "\n",
        "> 근데 대부분의 문서에서는 CNN, RNN은 DNN의 한 종류로 설명하고\n",
        "> 있어서요.. 이런 네트워크에서는 개념충돌이 옵니다.\n",
        "\n",
        "`#`\n",
        "\n",
        "`# 예시6` – RNN\n",
        "\n",
        "``` python\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = torch.nn.RNN(4,2)\n",
        "        self.linr = torch.nn.Linear(2,2) \n",
        "    def forward(self,X):\n",
        "        h,_ = self.rnn(X) \n",
        "        netout = self.linr(h)\n",
        "        return netout \n",
        "net = Net()     \n",
        "```\n",
        "\n",
        "-   ANN: O\n",
        "-   MLP: X\n",
        "-   DNN: X? (히든레이어가 1장이므로..)\n",
        "-   CNN: X (합성곱레이어가 없으므로)\n",
        "-   RNN: O\n",
        "\n",
        "> 이것도 비슷한 개념충돌\n",
        "\n",
        "`#`\n",
        "\n",
        "# A3. 학습\n",
        "\n",
        "`-` 모든 인공지능 관련 알고리즘은 아래의 분류로 가능함.\n",
        "\n",
        "|      **특징**       | **지도학습 (Supervised Learning)** | **비지도학습 (Unsupervised Learning)** | **강화학습 (Reinforcement Learning)**  |\n",
        "|:----------:|:------------------:|:-------------------:|:------------------:|\n",
        "|      **정의**       | 입력 데이터와 정답(레이블)을 사용  |           입력 데이터만 사용           |  에이전트가 환경과 상호작용하며 학습   |\n",
        "|      **목표**       |   입력에 대한 정확한 출력을 예측   |    데이터의 숨겨진 구조나 패턴 발견    | 최대 보상을 얻기 위한 최적의 정책 학습 |\n",
        "|      **예시**       |      이미지 분류, 스팸 필터링      |           군집화, 차원 축소            |         게임 플레이, 로봇 제어         |\n",
        "|  **주요 알고리즘**  |   선형 회귀, 로지스틱 회귀, SVM    |        K-평균, PCA, 오토인코더         |              Q-러닝, DQN               |\n",
        "|      **활용**       |             분류, 예측             |       데이터의 숨겨진 패턴 발견        |     복잡한 의사결정 문제 해결 가능     |\n",
        "| **데이터 요구사항** |       레이블링이 반드시 필요       |         많은 양의 데이터 필요          |     시뮬레이션 또는 실제 환경 필요     |\n",
        "\n",
        "`-` 그런데 분류가 애매한 것들이 점점 많아짐."
      ],
      "id": "813307f4-db43-44f8-8d03-1af0b333c966"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  }
}