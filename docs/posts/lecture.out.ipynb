{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05wk-1: xxxx\n",
        "\n",
        "최규빈  \n",
        "2025-01-01\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/lecture.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# Appendix – 신경망의 표현\n",
        "\n",
        "신경망의 표현: ${\\boldsymbol x} \\to \\hat{\\boldsymbol y}$ 로 가는 과정을\n",
        "그림으로 표현\n",
        "\n",
        "## 예제1: $\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(1)}} =\\underset{(n,1)}{\\hat{\\boldsymbol y}}$\n",
        "\n",
        "`-` 모든 observation과 가중치를 명시한 버전\n",
        "\n",
        "**(표현1)**\n",
        "\n",
        "![](https://guebin.github.io/DL2022/posts/II.%20DNN/2022-10-11-6wk-2_files/figure-html/cell-4-output-1.svg)\n",
        "\n",
        "-   단점: 똑같은 그림의 반복이 너무 많음\n",
        "\n",
        "`-` observation 반복을 생략한 버전들\n",
        "\n",
        "**(표현2)** 모든 $i$에 대하여 아래의 그림을 반복한다고 하면 (표현1)과\n",
        "같다.\n",
        "\n",
        "![](https://guebin.github.io/DL2022/posts/II.%20DNN/2022-10-11-6wk-2_files/figure-html/cell-5-output-1.svg)\n",
        "\n",
        "**(표현3)** 그런데 (표현2)에서 아래와 같이 $x_i$, $y_i$ 대신에 간단히\n",
        "$x$, $y$로 쓰는 경우도 많음\n",
        "\n",
        "![](https://guebin.github.io/DL2022/posts/II.%20DNN/2022-10-11-6wk-2_files/figure-html/cell-6-output-1.svg)\n",
        "\n",
        "`-` 1을 생략한 버전들\n",
        "\n",
        "**(표현4)** bais=False 대신에 bias=True를 주면 1을 생략할 수 있음\n",
        "\n",
        "![](https://guebin.github.io/DL2022/posts/II.%20DNN/2022-10-11-6wk-2_files/figure-html/cell-7-output-1.svg)\n",
        "\n",
        "**(표현4의 수정)** $\\hat{w}_1$대신에 $\\hat{w}$를 쓰는 것이 더 자연스러움\n",
        "\n",
        "![](https://guebin.github.io/DL2022/posts/II.%20DNN/2022-10-11-6wk-2_files/figure-html/cell-8-output-1.svg)\n",
        "\n",
        "**(표현5)** 선형변환의 결과는 아래와 같이 $u$로 표현하기도 한다.\n",
        "\n",
        "![](https://guebin.github.io/DL2022/posts/II.%20DNN/2022-10-11-6wk-2_files/figure-html/cell-9-output-1.svg)\n",
        "\n",
        "> 다이어그램은 그리는 사람의 취향에 따라 그리는 방법이 조금씩 다릅니다.\n",
        "> 즉 교재마다 달라요.\n",
        "\n",
        "## 예제2: $\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}} =\\underset{(n,1)}{\\hat{\\boldsymbol y}}$\n",
        "\n",
        "**참고: 코드로 표현**\n",
        "\n",
        "``` python\n",
        "torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=1,out_features=2),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features=2,out_features=1),\n",
        "    torch.nn.Sigmoid()\n",
        ")\n",
        "```\n",
        "\n",
        "`-` 이해를 위해서 예젠에 다루었던 아래의 상황을 고려하자.\n",
        "\n",
        "![](https://guebin.github.io/DL2024/posts/03wk-2_files/figure-html/cell-11-output-1.png)\n",
        "\n",
        "**(강의노트의 표현)**\n",
        "\n",
        "![](https://guebin.github.io/DL2022/posts/II.%20DNN/2022-10-11-6wk-2_files/figure-html/cell-10-output-1.svg)\n",
        "\n",
        "**(좀 더 일반화된 표현)** 상황을 일반화하면 아래와 같다.\n",
        "\n",
        "![](https://guebin.github.io/DL2022/posts/II.%20DNN/2022-10-11-6wk-2_files/figure-html/cell-11-output-1.svg)\n",
        "\n",
        "`*` Layer의 개념: ${\\bf X}$에서 $\\hat{\\boldsymbol y}$로 가는 과정은\n",
        "“선형변환+비선형변환”이 반복되는 구조이다. “선형변환+비선형변환”을\n",
        "하나의 세트로 보면 아래와 같이 표현할 수 있다.\n",
        "\n",
        "-   $\\underset{(n,1)}{\\bf X}  \\overset{l_1}{\\to} \\left( \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\right) \\overset{l_2}{\\to} \\left(\\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}\\right), \\quad  \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{net({\\bf X})}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}$\n",
        "\n",
        "이것을 다이어그램으로 표현한다면 아래와 같다.\n",
        "\n",
        "**(선형+비선형을 하나의 Layer로 묶은 표현)**\n",
        "\n",
        "![](https://guebin.github.io/DL2022/posts/II.%20DNN/2022-10-11-6wk-2_files/figure-html/cell-12-output-1.svg)\n",
        "\n",
        "***Layer를 세는 방법***\n",
        "\n",
        "-   제 방식: 학습가능한 파라메터가 몇층으로 있는지… \\<– 이것만\n",
        "    기억하세여\n",
        "-   일부 교재 설명: 입력층은 계산하지 않음, activation layer는 계산하지\n",
        "    않음. \\<– 무시하세요.. 이러면 헷갈립니다..\n",
        "-   위의 예제의 경우 `number of layer = 2` 이다.\n",
        "\n",
        "***Hidden Layer의 수를 세는 방법***\n",
        "\n",
        "-   제 방식: `Hidden Layer의 수 = Layer의 수 -1` \\<– 이걸 기억하세여..  \n",
        "-   일부 교재 설명:\n",
        "    `Layer의 수 = Hidden Layer의 수 + 출력층의 수 = Hidden Layer의 수 + 1`\n",
        "    \\<– 기억하지 마세여\n",
        "-   위의 예제의 경우 `number of hidden layer = 1` 이다.\n",
        "\n",
        "> **Important**\n",
        ">\n",
        "> 무조건 학습가능한 파라메터가 몇겹으로 있는지만 판단하세요. 딴거\n",
        "> 아무것도 생각하지마세여\n",
        ">\n",
        "> ``` python\n",
        "> ## 예시1 -- 2층 (히든레이어는 1층)\n",
        "> torch.nn.Sequential(\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층\n",
        ">     torch.nn.ReLU(),\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층\n",
        "> )\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> ## 예시2 -- 2층 (히든레이어는 1층)\n",
        "> torch.nn.Sequential(\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층\n",
        ">     torch.nn.ReLU(),\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층\n",
        ">     torch.nn.Sigmoid(),\n",
        "> )\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> ## 예시3 -- 1층 (히든레이어는 없음!!)\n",
        "> torch.nn.Sequential(\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층\n",
        "> ) \n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> ## 예시4 -- 1층 (히든레이어는 없음!!)\n",
        "> torch.nn.Sequential(\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층\n",
        ">     torch.nn.Sigmoid()\n",
        "> ) \n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> ## 예시5 -- 3층 (히든레이어는 2층)\n",
        "> torch.nn.Sequential(\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층\n",
        ">     torch.nn.Sigmoid()\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층\n",
        ">     torch.nn.Sigmoid()\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층    \n",
        "> ) \n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> ## 예시6 -- 3층 (히든레이어는 2층)\n",
        "> torch.nn.Sequential(\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층\n",
        ">     torch.nn.ReLU()\n",
        ">     torch.nn.Dropout(??)\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층\n",
        ">     torch.nn.ReLU()\n",
        ">     torch.nn.Dropout(??)\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층  \n",
        ">     torch.nn.Sigmoid()\n",
        "> ) \n",
        "> ```\n",
        "\n",
        "> **Important**\n",
        ">\n",
        "> 문헌에 따라서 레이어를 세는 개념이 제가 설명한 방식과 다른경우가\n",
        "> 있습니다. 제가 설명한 방식보다 1씩 더해서 셉니다. 즉 아래의 경우\n",
        "> 레이어를 3개로 카운트합니다.\n",
        ">\n",
        "> ``` python\n",
        "> ## 예시1 -- 문헌에 따라 3층으로 세는 경우가 있음 (히든레이어는 1층)\n",
        "> torch.nn.Sequential(\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층\n",
        ">     torch.nn.ReLU(),\n",
        ">     torch.nn.Linear(??,??), ## <-- 학습해야할 가중치가 있는 층\n",
        ">     torch.nn.Sigmoid()\n",
        "> )\n",
        "> ```\n",
        ">\n",
        "> 예를 들어\n",
        "> [여기](https://en.wikipedia.org/wiki/Multilayer_perceptron#Layers)에서는\n",
        "> 위의 경우 레이어는 3개라고 설명하고 있습니다. 이러한 카운팅은\n",
        "> **“무시”하세요. 제가 설명한 방식이 맞아요.** [이\n",
        "> 링크](https://en.wikipedia.org/wiki/Multilayer_perceptron#Layers)\n",
        "> 잘못(?) 나와있는 이유는 아래와 같습니다.\n",
        ">\n",
        "> `-` 진짜 예전에 MLP를 소개할 초창기에서는 위의 경우 Layer를 3개로\n",
        "> 셌음. (Rosenblatt et al. 1962)\n",
        ">\n",
        "> `-` 그런데 요즘은 그렇게 안셈.. (그리고 애초에 MLP라는 용어도 잘\n",
        "> 안쓰죠..)\n",
        ">\n",
        "> 참고로 히든레이어의 수는 예전방식이나 지금방식이나 동일하게\n",
        "> 카운트하므로 히든레이어만 세면 혼돈이 없습니다.\n",
        "\n",
        "`*` node의 개념: $u\\to v$로 가는 쌍을 간단히 노드라는 개념을 이용하여\n",
        "나타낼 수 있음.\n",
        "\n",
        "**(노드의 개념이 포함된 그림)**\n",
        "\n",
        "![](https://guebin.github.io/DL2022/posts/II.%20DNN/2022-10-11-6wk-2_files/figure-html/cell-13-output-1.svg)\n",
        "\n",
        "여기에서 `node의 숫자 = feature의 숫자`와 같이 이해할 수 있다. 즉 아래와\n",
        "같이 이해할 수 있다.\n",
        "\n",
        "**(“number of nodes = number of features”로 이해한 그림)**\n",
        "\n",
        "![](https://guebin.github.io/DL2022/posts/II.%20DNN/2022-10-11-6wk-2_files/figure-html/cell-14-output-1.svg)\n",
        "\n",
        "> 다이어그램의 표현방식은 교재마다 달라서 모든 예시를 달달 외울 필요는\n",
        "> 없습니다. 다만 임의의 다이어그램을 보고 대응하는 네트워크를 pytorch로\n",
        "> 구현하는 능력은 매우 중요합니다.\n",
        "\n",
        "## 예제3: $\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}$\n",
        "\n",
        "**(다이어그램표현)**\n",
        "\n",
        "![](https://guebin.github.io/DL2022/posts/II.%20DNN/2022-10-11-6wk-2_files/figure-html/cell-15-output-1.svg)\n",
        "\n",
        "-   Layer0,1,2 대신에 Input Layer, Hidden Layer, Output Layer로 표현함\n",
        "\n",
        "`-` 위의 다이어그램에 대응하는 코드\n",
        "\n",
        "``` python\n",
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=28*28*1,out_features=32),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features=32,out_features=1),\n",
        "    torch.nn.Sigmoid() \n",
        ")\n",
        "```\n",
        "\n",
        "Rosenblatt, Frank et al. 1962. *Principles of Neurodynamics: Perceptrons\n",
        "and the Theory of Brain Mechanisms*. Vol. 55. Spartan books Washington,\n",
        "DC."
      ],
      "id": "c87bbf3a-5561-4b84-9625-6b086d81d143"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  }
}