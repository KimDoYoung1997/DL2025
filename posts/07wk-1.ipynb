{
 "cells": [
  {
   "cell_type": "raw",
   "id": "aeff2ac3-8f9f-4111-ab0c-7e897c0b0ea5",
   "metadata": {
    "id": "87b5cded-346b-4915-acf5-b5ec93a5207d",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"07wk-1: (합성곱신경망) -- CNN 자랑, CNN 핵심레이어\"\n",
    "author: \"최규빈\"\n",
    "date: \"04/16/2025\"\n",
    "draft: false\n",
    "freeze: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739324d-6331-47b4-8aae-da740055ff9d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/06wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcaeff1-7791-48ed-9c60-b954ffe6b7f2",
   "metadata": {
    "id": "4d47a7c9",
    "tags": []
   },
   "source": [
    "# 1. 강의영상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854076ca-0234-427b-9a95-147a041cf58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# {{<video https://youtu.be/playlist?list=PLQqh36zP38-wcPiCEdYML9-6-Xv5RVbso&si=BbNo6mwCHqwOV0FS>}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c59bc1-1a1d-4287-b55a-a26104664fe0",
   "metadata": {},
   "source": [
    "# 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229dfe04-bb02-4a02-81e4-c6fdd9eb2f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758b9fdc-63aa-4526-a12e-42a784516343",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4.5, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a66400-3cdc-42a7-9c81-2ec4eee68ef2",
   "metadata": {},
   "source": [
    "# 3. CNN 자랑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40719670-b070-4639-b689-48e886e8b945",
   "metadata": {},
   "source": [
    "## A. 성능좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446a9ea-72f1-4a79-955c-73374fdf7c52",
   "metadata": {},
   "source": [
    "*Fashion MNIST*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19ffc154-e4b6-4e09-9c32-73110128d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True)\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, range(5000))\n",
    "test_dataset = torch.utils.data.Subset(test_dataset, range(1000))\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "X = torch.stack([to_tensor(img) for img, lbl in train_dataset]).to(\"cuda:0\")\n",
    "y = torch.tensor([lbl for img, lbl in train_dataset])\n",
    "y = torch.nn.functional.one_hot(y).float().to(\"cuda:0\")\n",
    "XX = torch.stack([to_tensor(img) for img, lbl in test_dataset]).to(\"cuda:0\")\n",
    "yy = torch.tensor([lbl for img, lbl in test_dataset])\n",
    "yy = torch.nn.functional.one_hot(yy).float().to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e698e-94d9-4598-a9ae-84ed1044c766",
   "metadata": {},
   "source": [
    "*발악수준으로 설계한 신경망*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5290e2d-e9d9-4641-92fd-7e6df5b68ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784,2048),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(2048,10)\n",
    ").to(\"cuda\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "963b8ded-f1e7-48be-8975-3ba38bfac4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoc in range(1,500):\n",
    "    #1\n",
    "    logits = net(X)\n",
    "    #2\n",
    "    loss = loss_fn(logits, y) \n",
    "    #3\n",
    "    loss.backward()\n",
    "    #4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0972367a-51bb-47c6-8ee3-cd177c968dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(X).argmax(axis=1) == y.argmax(axis=1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b95f70ad-2b90-4ecb-9610-90a91c862c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8520, device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(XX).argmax(axis=1) == yy.argmax(axis=1)).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411382b-477e-4aec-888b-d230844df9e1",
   "metadata": {},
   "source": [
    "*대충대충 설계한 합성곱신경망*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70a00a09-6d44-43de-9395-f68d7ca25b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(2),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(2704,10),\n",
    ").to(\"cuda\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7b598c9-e55b-4d88-a2a9-1974d20f0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoc in range(1,500):\n",
    "    #1\n",
    "    logits = net(X)\n",
    "    #2\n",
    "    loss = loss_fn(logits, y) \n",
    "    #3\n",
    "    loss.backward()\n",
    "    #4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84e56cf1-abf9-49f8-82cf-2cd9e739c7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9466, device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(X).argmax(axis=1) == y.argmax(axis=1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ca554ed-6556-4c13-bf7b-0de533ceb487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8710, device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(XX).argmax(axis=1) == yy.argmax(axis=1)).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508ed6f-d728-459d-8faf-95c071e17921",
   "metadata": {},
   "source": [
    "## B. 파라메터적음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f7ea0eee-5dff-4d54-a05b-1b9681d6aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784,2048),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(2048,10)\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(2),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(2704,10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7304bb47-7550-41c5-baae-aa92909d8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1_params = list(net1.parameters())\n",
    "net2_params = list(net2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0ef7b74d-bff2-4f7b-8ba9-58178603b674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 784])\n",
      "torch.Size([2048])\n",
      "torch.Size([10, 2048])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for params in net1_params:\n",
    "    print(params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "83ee0d71-65cf-4e41-95c2-26592e2c5b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1628170"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2048*784 + 2048 + 10*2048 + 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3fd0625a-44ca-4a30-9316-dc3cc6c1d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 2, 2])\n",
      "torch.Size([16])\n",
      "torch.Size([10, 2704])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for params in net2_params:\n",
    "    print(params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "54d4ed1d-50dc-418a-9f02-c0759855f566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27130"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*1*2*2 + 16 + 10*2704 + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615fc25-fbb6-47f8-861b-14fa69af4371",
   "metadata": {},
   "source": [
    "## C. 유명함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf82636-e42f-4063-805f-df879b295f9d",
   "metadata": {},
   "source": [
    "`-` <https://brunch.co.kr/@hvnpoet/109> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3724d0-c678-4508-966a-ea26f4638e87",
   "metadata": {},
   "source": [
    "# 4. CNN 핵심레이어 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba3258-94bc-4ef4-bffc-c04ddbb2373a",
   "metadata": {},
   "source": [
    "## A. `torch.nn.ReLU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2159d1a-c28f-45ce-bd75-c1929ee5da42",
   "metadata": {},
   "source": [
    "**(예시1) 연산방법**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ddfe4d3f-d4d8-4c34-ac5c-8de819f3ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(1,1,5,5) # 흑백이미지한장\n",
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40beef3a-f5f7-494d-b34d-f6b5a33d45e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.7009,  0.7528,  0.1870,  0.2400, -0.3679],\n",
       "          [ 2.6662,  1.3107,  0.9368, -0.8709, -1.7125],\n",
       "          [-1.6364, -0.8744,  0.6034,  0.0990,  1.7091],\n",
       "          [ 0.2130, -0.0287, -0.2431,  1.7716,  1.1965],\n",
       "          [-0.3508, -0.1929,  1.1716, -0.6615,  1.3645]]]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img # - 값도 있지만 이미지라고 생각하자.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4fd92056-55e6-4f61-96a7-722d890c7ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.7009, 0.7528, 0.1870, 0.2400, 0.0000],\n",
       "          [2.6662, 1.3107, 0.9368, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.6034, 0.0990, 1.7091],\n",
       "          [0.2130, 0.0000, 0.0000, 1.7716, 1.1965],\n",
       "          [0.0000, 0.0000, 1.1716, 0.0000, 1.3645]]]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80678c3-133a-43ed-bbbf-05ecec03b81f",
   "metadata": {},
   "source": [
    "## B. `torch.nn.MaxPool2d`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f4e07c-3f5e-4d16-8d37-53183a1bf962",
   "metadata": {},
   "source": [
    "**(예시1) 연산방법, kernel_size 의 의미**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c4c00350-ee51-4c86-bbdf-fcbf8c66daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(1,1,4,4) # 흑백이미지한장\n",
    "mp = torch.nn.MaxPool2d(kernel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "630dbbd6-da23-4339-bbc2-b9efa8dca8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9650, 0.4934, 0.3260, 0.4669],\n",
       "          [0.2401, 0.5172, 0.3875, 0.9785],\n",
       "          [0.7401, 0.5297, 0.3935, 0.2731],\n",
       "          [0.3243, 0.1672, 0.3824, 0.5780]]]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1dbbbc48-4ebc-4412-8a92-8bbbd93c7816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9650, 0.9785],\n",
       "          [0.7401, 0.5780]]]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a8c0c-f341-4d14-af49-d545540f3b6d",
   "metadata": {},
   "source": [
    "**(예시2) 이미지크기와 딱 맞지않는 커널일경우?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fccb5604-0db5-4948-bd95-51ae23efabd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(1,1,5,5) # 흑백이미지한장\n",
    "mp = torch.nn.MaxPool2d(kernel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b4cb5fa2-1eac-4158-9258-6cd618d5a9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8015, 0.9923, 0.5175, 0.2965, 0.7411],\n",
       "          [0.8369, 0.2625, 0.9127, 0.2559, 0.1240],\n",
       "          [0.8461, 0.2436, 0.4713, 0.5677, 0.8162],\n",
       "          [0.0301, 0.2276, 0.2605, 0.6200, 0.1242],\n",
       "          [0.1616, 0.7613, 0.6406, 0.2210, 0.2042]]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb1776fa-5b7f-46ad-8b01-4b72d0226782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9923, 0.9127],\n",
       "          [0.8461, 0.6200]]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd602009-ca63-45aa-b4c5-f1dfdfcd0426",
   "metadata": {},
   "source": [
    "**(예시3) 정사각형이 아닌 커널**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a478ae0-dfdd-48d7-8edd-3b36ef845cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(1,1,4,4) # 흑백이미지한장\n",
    "mp = torch.nn.MaxPool2d(kernel_size=(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "830cdd06-024c-43cf-b431-10a10a59b568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2001, 0.1130, 0.2961, 0.9473],\n",
       "          [0.2944, 0.8712, 0.8057, 0.4017],\n",
       "          [0.0661, 0.9816, 0.4964, 0.0813],\n",
       "          [0.5853, 0.0936, 0.1827, 0.2528]]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1f17e1c-792a-4166-b066-b60a1dd58740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9473],\n",
       "          [0.9816]]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc936b-c8a6-4c3c-a6d4-5d3e6ddb6e88",
   "metadata": {},
   "source": [
    "## C. `torch.nn.Conv2d`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9247218-9995-4ded-ab65-1820fe14ccba",
   "metadata": {},
   "source": [
    "**(예시1) 연산방법, stride=2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e43be7af-c9ee-44d7-adab-5d2586cb251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(1,1,4,4) # 흑백이미지한장\n",
    "conv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2,stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ee807862-2b35-43c5-9f33-c118adb00375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6455, 0.0018, 0.8217, 0.7056],\n",
       "          [0.7984, 0.5098, 0.4589, 0.6528],\n",
       "          [0.3544, 0.5795, 0.1119, 0.8301],\n",
       "          [0.0376, 0.8687, 0.2402, 0.8886]]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "671587db-11c0-43d6-b7c4-436982b66a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6272, 0.8083],\n",
       "          [0.2748, 0.4018]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d1a93-392d-4134-ba4d-fdb8649629ba",
   "metadata": {},
   "source": [
    "??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9c4b07ff-2ca2-45ec-ac11-c7f073d1d596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[[[ 0.3506,  0.4990],\n",
       "           [ 0.4772, -0.4890]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2682], requires_grad=True))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight, conv.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f1a9d7be-940f-439c-bcb4-5853d72c50b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0690], grad_fn=<AddBackward0>),\n",
       " tensor([[[[1.0690, 2.7447],\n",
       "           [7.7718, 9.4475]]]], grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img[:,  :,  :2,  :2] * conv.weight.data).sum() + conv.bias, conv(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "717081f4-3d98-4dc4-9b98-f29d719d4a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.7447], grad_fn=<AddBackward0>),\n",
       " tensor([[[[1.0690, 2.7447],\n",
       "           [7.7718, 9.4475]]]], grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img[:,  :,  :2,  2:] * conv.weight.data).sum() + conv.bias, conv(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "94278a54-352f-4d0c-b0f7-c8454f32f9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7.7718], grad_fn=<AddBackward0>),\n",
       " tensor([[[[1.0690, 2.7447],\n",
       "           [7.7718, 9.4475]]]], grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img[:,  :,  2:,  :2] * conv.weight.data).sum() + conv.bias, conv(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ec5e5316-05e4-4917-9813-4e318dcf656a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([9.4475], grad_fn=<AddBackward0>),\n",
       " tensor([[[[1.0690, 2.7447],\n",
       "           [7.7718, 9.4475]]]], grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img[:,  :,  2:,  2:] * conv.weight.data).sum() + conv.bias, conv(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0e6765-fbd9-40ad-827e-988fe372b1f0",
   "metadata": {},
   "source": [
    "**(예시2) 이동평균**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4182d95a-1e60-4f60-aa39-fb5e1100b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.arange(1,17).float().reshape(1,1,4,4)\n",
    "conv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2,stride=1,bias=False)\n",
    "conv.weight.data = conv.weight.data*0+1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a60186b1-74b3-41e8-b4e5-3e0201674d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "76d90240-4e7c-421b-be38-91dcf25d2104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.5000,  4.5000,  5.5000],\n",
       "          [ 7.5000,  8.5000,  9.5000],\n",
       "          [11.5000, 12.5000, 13.5000]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f7bba-255e-4a92-8727-5afe8858ef9e",
   "metadata": {},
   "source": [
    "**(예시3) 2개의 이미지**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0cb6c677-8e2a-4f87-8cee-62c6e1ebb4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.arange(1,33).float().reshape(2,1,4,4)\n",
    "conv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2,stride=1,bias=False)\n",
    "conv.weight.data = conv.weight.data*0+1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6bcb5b4f-d3e5-4eaa-a29a-fdf3e445204d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]],\n",
       "\n",
       "\n",
       "        [[[17., 18., 19., 20.],\n",
       "          [21., 22., 23., 24.],\n",
       "          [25., 26., 27., 28.],\n",
       "          [29., 30., 31., 32.]]]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "284a2078-6308-49e2-a1f7-7775bb2a44ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.5000,  4.5000,  5.5000],\n",
       "          [ 7.5000,  8.5000,  9.5000],\n",
       "          [11.5000, 12.5000, 13.5000]]],\n",
       "\n",
       "\n",
       "        [[[19.5000, 20.5000, 21.5000],\n",
       "          [23.5000, 24.5000, 25.5000],\n",
       "          [27.5000, 28.5000, 29.5000]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb8f7a2-aa8f-4bda-9c8e-97a4c5208b99",
   "metadata": {},
   "source": [
    "**(예시4) 2개의 out_channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "0037cb52-49e7-4ca9-846d-57c34f1119ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.arange(1,33).float().reshape(2,1,4,4)\n",
    "conv = torch.nn.Conv2d(in_channels=1,out_channels=2,kernel_size=2,stride=1,bias=False)\n",
    "conv.weight.data[0] = conv.weight.data[0]*0 + 1/4\n",
    "conv.weight.data[1] = conv.weight.data[1]*0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "50ad8b5e-dee2-4d2c-9891-98d1fbb7bc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]],\n",
       "\n",
       "\n",
       "        [[[17., 18., 19., 20.],\n",
       "          [21., 22., 23., 24.],\n",
       "          [25., 26., 27., 28.],\n",
       "          [29., 30., 31., 32.]]]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0b929a67-732e-45cc-b7e0-baacf44aceec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.5000,  4.5000,  5.5000],\n",
       "          [ 7.5000,  8.5000,  9.5000],\n",
       "          [11.5000, 12.5000, 13.5000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[19.5000, 20.5000, 21.5000],\n",
       "          [23.5000, 24.5000, 25.5000],\n",
       "          [27.5000, 28.5000, 29.5000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
