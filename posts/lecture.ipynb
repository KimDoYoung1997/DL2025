{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a38c22cf-74ca-41d2-95ba-b973a0263535",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"lecture\"\n",
    "author: \"최규빈\"\n",
    "date: \"05/19/2025\"\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b27a7a-19b0-4cf3-bcf4-8db99f55036d",
   "metadata": {
    "id": "e67ab8e0"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/09wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34257653-1a9b-4b05-8a0f-fa267a77c995",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 강의영상 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "99d156c1-b7cd-4735-b27b-ed4bbe961181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# {{<video https://youtu.be/playlist?list=PLQqh36zP38-zEjn2m8H8hMCHsQK8udE27&si=Sy-lnw4Kq56SRggu >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70183d4-475c-4d1c-beb3-3b32aa565142",
   "metadata": {},
   "source": [
    "# 2. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e5af06a9-81d1-446e-baca-bdb9b5e722e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0708b5fa-4ed9-48c4-a463-3cf3402b303e",
   "metadata": {},
   "source": [
    "# 3. 강화학습 Intro "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7c4cae-4d92-4b12-b9af-2c20a2dc4ac1",
   "metadata": {},
   "source": [
    "`-` 강화학습(대충설명): 어떠한 \"(게임)환경\"이 있을때 거기서 \"뭘 할지\"를 학습하는 과업"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0be0b7a3-240d-48f0-88b2-5390b19802a5",
   "metadata": {},
   "source": [
    "![그림1: 셔튼(@sutton1998reinforcement)의 교재에서 발췌한 그림, 되게 유명한 그림이에요](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda26e7-ed3c-4fd8-9ab4-af00c4bb8f4d",
   "metadata": {},
   "source": [
    "`-` 딥마인드: breakout $\\to$ 알파고 \n",
    "\n",
    "- <https://www.youtube.com/watch?v=TmPfTpjtdgg>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a9f3176-0158-46ed-bec5-435ef28aa879",
   "metadata": {},
   "source": [
    "![그림2: 벽돌깨기](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig2.png?raw=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad48af23-475e-45b5-b90b-738190008592",
   "metadata": {},
   "source": [
    "`-` 강화학습에서 \"강화\"는 뭘 강화한다는것일까? \n",
    "\n",
    "- <https://k9connoisseur.com/blogs/news/positive-reinforcement-dog-training> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411206af-ca04-4739-be37-88c014fb6dad",
   "metadata": {},
   "source": [
    "`-` 강화학습 미래? (이거 잘하면 먹고 살 수 있을까?) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062957eb-641e-4801-b66b-d04e8b93cd15",
   "metadata": {},
   "source": [
    "# 4. Bandit 게임 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e49c0-5f27-4629-91bc-5665bf6194c1",
   "metadata": {},
   "source": [
    "`-` 문제설명: 두 개의 버튼이 있다. `버튼0`을 누르면 1의 보상을, `버튼1`을 누르면 10의 보상을 준다고 가정 \n",
    "\n",
    "- Agent: 버튼0을 누르거나,버튼1을 누르는 존재 \n",
    "- Env: Agent의 Action을 바탕으로 Reward를 주는 존재\n",
    "\n",
    "> 주의: 이 문제 상황에서 state는 없음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08bbeb-fb13-446d-88c9-5a374fcac7eb",
   "metadata": {},
   "source": [
    "`-` 생성형AI로 위의 상황을 설명한것 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a762101-f292-47eb-8ecb-82dee6c8877a",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; gap: 10px;\">\n",
    "  <div style=\"flex: 1; text-align: center;\">\n",
    "    <img src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gpt.png?raw=true\" style=\"width: 100%;\">\n",
    "    <p>(a) 챗지피티로 생성한 그림</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; text-align: center;\">\n",
    "    <img src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gemini.png?raw=true\" style=\"width: 100%;\">\n",
    "    <p>(b) 제미나이로 생성한 그림</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; text-align: center;\">\n",
    "    <img src=\"https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-perplexity.png?raw=true\" style=\"width: 100%;\">\n",
    "    <p>(c) 퍼플렉시티로 생성한 그림</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe917ee6-6754-4383-a6fa-36b6aa09357a",
   "metadata": {},
   "source": [
    "- 클로드로 생성: <https://claude.ai/public/artifacts/1f52fcb2-ef08-4af1-8cf8-4a497d7bcc5f>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd67038-356e-4e0d-a1ed-7066e7914084",
   "metadata": {},
   "source": [
    "`-` 게임진행양상\n",
    "\n",
    "- 처음에는 아는게 없음. 일단 \"아무거나\" 눌러보자. (\"에이전트가 랜덤액션을 한다\" 고 표현함 )\n",
    "- 한 20번 정도 눌러보면서 결과를 관찰함 (\"에이전트가 경험을 축적한다\"고 표현함) \n",
    "- 버튼0을 누를때는 1점, 버튼1을 누를때는 10점을 준다는 사실을 깨달음. (\"에이전트가 환경을 이해했다\"고 표현함)\n",
    "- 버튼1을 누르는게 나한테 이득이 라는 사실을 깨달음. (\"에이전트가 최적의 정책을 학습했다\" 고 표현함)\n",
    "- 이제부터 무조건 버튼1만 누름 $\\to$ 게임 클리어 (\"강화학습 성공\"이라 표현할 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c57e6-030e-4e77-b762-214446d74196",
   "metadata": {},
   "source": [
    "`-` 어떻게 버튼1을 누르는게 이득이라는 사실을 아는거지? $\\to$ 아래와 같은 테이블을 만들면 된다. (`q_table`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f832cbd-0720-49b6-bd51-c1c26baa2e8a",
   "metadata": {},
   "source": [
    "|| Action0 | Action1 | \n",
    "|:-:|:-:|:-:|\n",
    "|State0 | mean(Reward \\| State0, Action0) |mean(Reward \\| State0, Action1)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e8fa90-e56c-4970-8589-ced0202cc22b",
   "metadata": {},
   "source": [
    "# 5. 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a1575-a5eb-4fe1-8e10-a696d53ee852",
   "metadata": {},
   "source": [
    "## A. 대충 개념만 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "c2c71723-14b2-4c3e-8907-324ba9c930f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [0,1] \n",
    "actions_deque = collections.deque(maxlen=200)\n",
    "rewards_deque = collections.deque(maxlen=200)\n",
    "#---#\n",
    "for _ in range(10):\n",
    "    action = np.random.choice(action_space)\n",
    "    if action ==0: \n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 10\n",
    "    actions_deque.append(action)\n",
    "    rewards_deque.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "67719b43-0dae-4577-8f73-60d74c69716d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([1, 0, 1, 0, 0, 1, 1, 0, 1, 1], maxlen=200)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "d24af941-b48d-4b25-81c0-6628b7f19219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([10, 1, 10, 1, 1, 10, 10, 1, 10, 10], maxlen=200)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "4018124b-9bd2-4c50-ac2f-c7c44e3bcf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_numpy = np.array(actions_deque)\n",
    "rewards_numpy = np.array(rewards_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "9cc6c620-b481-496a-aa90-38768e6af971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., 10.])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q0 = rewards_numpy[actions_numpy==0].mean()\n",
    "q1 = rewards_numpy[actions_numpy==1].mean()\n",
    "q_table = np.array([q0,q1])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "09244558-e9b5-4fc3-86b1-1370da86d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---#\n",
    "for _ in range(5):\n",
    "    #action = np.random.choice(action_space)\n",
    "    action = q_table.argmax()\n",
    "    if action ==0: \n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 10\n",
    "    actions_deque.append(action)\n",
    "    rewards_deque.append(reward)\n",
    "    actions_numpy = np.array(actions_deque)\n",
    "    rewards_numpy = np.array(rewards_deque)\n",
    "    q0 = rewards_numpy[actions_numpy==0].mean()\n",
    "    q1 = rewards_numpy[actions_numpy==1].mean()\n",
    "    q_table = np.array([q0,q1])\n",
    "    q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "1ece00ec-2ce2-47c3-8dc5-426f5ec74676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GameClear\n"
     ]
    }
   ],
   "source": [
    "if rewards_numpy[-5:].mean() > 9:\n",
    "    print(\"GameClear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6065a1-fb75-4350-9297-bde6c82c4e0a",
   "metadata": {},
   "source": [
    "## B. 클래스를 이용한 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "61984d91-1f69-457f-9b70-d4b40fe064ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batdit():\n",
    "    def __init__(self):\n",
    "        self.reward = None \n",
    "    def step(self,action):\n",
    "        if action == 0:\n",
    "            self.reward = 1\n",
    "        elif action == 1:\n",
    "            self.reward = 10\n",
    "        return self.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "87e52697-88bf-4f45-b3d3-ed2824475a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.n_experiences = 0 \n",
    "        self.action_space = [0,1]\n",
    "        self.action = None \n",
    "        self.actions_deque = collections.deque(maxlen=500)\n",
    "        self.actions_numpy = np.array(self.actions_deque)\n",
    "        self.reward = None \n",
    "        self.rewards_deque = collections.deque(maxlen=500)\n",
    "        self.rewards_numpy = np.array(self.rewards_deque)\n",
    "        self.q_table = None\n",
    "    def act(self):\n",
    "        if self.n_experiences < 20:\n",
    "            self.action = np.random.choice(self.action_space)\n",
    "        else: \n",
    "            self.action = self.q_table.argmax()\n",
    "        print(f\"버튼{self.action}누름\")\n",
    "    def save_experience(self):\n",
    "        self.n_experiences = self.n_experiences + 1\n",
    "        self.actions_deque.append(self.action)\n",
    "        self.rewards_deque.append(self.reward)\n",
    "        self.actions_numpy = np.array(self.actions_deque)\n",
    "        self.rewards_numpy = np.array(self.rewards_deque)\n",
    "    def learn(self):\n",
    "        if self.n_experiences < 20:\n",
    "            pass\n",
    "        else: \n",
    "            q0 = self.rewards_numpy[self.actions_numpy == 0].mean()\n",
    "            q1 = self.rewards_numpy[self.actions_numpy == 1].mean()\n",
    "            self.q_table = np.array([q0,q1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "793b2cf2-5b46-467f-84a3-1eca97f76dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Batdit()\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "50c41363-bbfa-41c0-b7a5-0affe4acd346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "버튼0누름\n"
     ]
    }
   ],
   "source": [
    "agent.act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "37f57aef-eadd-487b-a9f6-7226909f4fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "버튼0누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼0누름\n",
      "버튼0누름\n",
      "버튼1누름\n",
      "버튼0누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼0누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼0누름\n",
      "버튼0누름\n",
      "버튼0누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "버튼1누름\n",
      "게임클리어\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    #1. 행동\n",
    "    agent.act()\n",
    "    #2. 보상\n",
    "    agent.reward = env.step(agent.action)\n",
    "    #3. 저장 & 학습 \n",
    "    agent.save_experience()\n",
    "    agent.learn()\n",
    "    #---#\n",
    "    if (agent.n_experiences > 20) and (agent.rewards_numpy[-20:].mean() >9):\n",
    "        print(\"게임클리어\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
