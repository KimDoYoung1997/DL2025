{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7a9ee665-7172-4c03-97f3-96393b519497",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"14wk-1: (강화학습) -- 4x4 Grid World 환경의 이해\"\n",
    "author: \"최규빈\"\n",
    "date: \"06/04/2025\"\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860568c9-90af-4b0e-afc7-6040ce1592e7",
   "metadata": {
    "id": "e67ab8e0"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/14wk-1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782deb0-080f-45f8-b122-c6eb7285ed47",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 강의영상 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b737e5-9cc4-4734-afd4-1d6f2fabacc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "{{<video https://youtu.be/playlist?list=PLQqh36zP38-xtp-2udbCeLzUvHSvI4X3y&si=vYqsA5Sffbs7C0xD >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec70d12-2d53-494e-90a5-bc3b07727127",
   "metadata": {},
   "source": [
    "# 2. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d667404-aced-49db-b8c1-bb4e41eba4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "#---#\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9c3d76-670a-4283-997c-cd4f5d373781",
   "metadata": {},
   "source": [
    "# 3. 지난시간 코드 복습 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ad7cd-f74f-4d0a-9f48-41b4b234e676",
   "metadata": {},
   "source": [
    "`-` 클래스선언 \n",
    "\n",
    "- 수정사항: (1)  deque의 maxlen을 500000 으로 조정 (2) print하는 코드를 주석처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51ea8f17-ff1a-47a6-b30e-9c2e0ca9fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.a2d = {\n",
    "            0: np.array([0,1]), # →\n",
    "            1: np.array([0,-1]), # ←  \n",
    "            2: np.array([1,0]),  # ↓\n",
    "            3: np.array([-1,0])  # ↑\n",
    "        }\n",
    "        self.state_space = gym.spaces.MultiDiscrete([4,4])\n",
    "        self.state = np.array([0,0])\n",
    "        self.reward = None\n",
    "        self.terminated = False\n",
    "    def step(self,action):\n",
    "        self.state = self.state + self.a2d[action]\n",
    "        s1,s2 = self.state\n",
    "        if (s1==3) and (s2==3):\n",
    "            self.reward = 100 \n",
    "            self.terminated = True\n",
    "        elif self.state in self.state_space:\n",
    "            self.reward = -1 \n",
    "            self.terminated = False\n",
    "        else:\n",
    "            self.reward = -10\n",
    "            self.terminated = True\n",
    "        # print(\n",
    "        #     f\"action = {action}\\t\"\n",
    "        #     f\"state = {self.state - self.a2d[action]} -> {self.state}\\t\"\n",
    "        #     f\"reward = {self.reward}\\t\"\n",
    "        #     f\"termiated = {self.terminated}\"\n",
    "        # )            \n",
    "        return self.state, self.reward, self.terminated\n",
    "    def reset(self):\n",
    "        self.state = np.array([0,0])\n",
    "        self.terminated = False\n",
    "        return self.state\n",
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.state = np.array([0,0]) \n",
    "        self.action = None \n",
    "        self.reward = None \n",
    "        self.next_state = None\n",
    "        self.terminated = None\n",
    "        #---#\n",
    "        self.states = collections.deque(maxlen=500000)\n",
    "        self.actions = collections.deque(maxlen=500000)\n",
    "        self.rewards = collections.deque(maxlen=500000)\n",
    "        self.next_states = collections.deque(maxlen=500000)\n",
    "        self.terminations = collections.deque(maxlen=500000)\n",
    "        #---#\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.n_experience = 0\n",
    "    def act(self):\n",
    "        self.action = self.action_space.sample()\n",
    "    def save_experience(self):\n",
    "        self.states.append(self.state)\n",
    "        self.actions.append(self.action)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.next_states.append(self.next_state)\n",
    "        self.terminations.append(self.terminated)\n",
    "        self.n_experience = self.n_experience + 1\n",
    "    def learn(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d55f9-775e-4645-b06b-1efd64e23572",
   "metadata": {},
   "source": [
    "`-` 메인코드 \n",
    "\n",
    "- 수정사항: 가독성을 위해 에피소드가 진행되는 for문의 구조를 수정함 (특히 step4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1782b1-b299-4364-b358-374f404f20a7",
   "metadata": {},
   "source": [
    "```Python\n",
    "player = RandomAgent()\n",
    "env = GridWorld()\n",
    "scores = [] \n",
    "score = 0 \n",
    "#\n",
    "for e in range(1,100):\n",
    "    #---에피소드시작---#\n",
    "    while True:\n",
    "        # step1 -- 액션선택\n",
    "        player.act()\n",
    "        # step2 -- 환경반응 \n",
    "        player.next_state, player.reward, player.terminated = env.step(player.action)\n",
    "        # step3 -- 경험기록 & 학습 \n",
    "        player.save_experience()\n",
    "        player.learn()\n",
    "        # step4 --종료 조건 체크 & 후속 처리\n",
    "        if env.terminated:\n",
    "            score = score + player.reward\n",
    "            scores.append(score)\n",
    "            score = 0 \n",
    "            player.state = env.reset() \n",
    "            print(f\"---에피소드{e}종료---\")\n",
    "            break\n",
    "        else: \n",
    "            score = score + player.reward\n",
    "            scores.append(score)            \n",
    "            player.state = player.next_state\n",
    "    #---에피소드끝---#\n",
    "    if scores[-1] > 0:\n",
    "        break\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5b849-087c-4b3b-be12-9f3e44d42749",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "마음에 들지 않지만 꼭 외워야 하는것 \n",
    "\n",
    "1. `env.step`은 항상 next_state, reward, terminated, truncated, info 를 리턴한다. -- 짐나지엄 라이브러리 규격때문\n",
    "2. `env.reset`은 환경을 초기화할 뿐만 아니라, state, info를 반환하는 기능도 있다.  -- 짐나지엄 라이브러리 규격때문\n",
    "3. `player`는 항상 `state`와 `next_state`를 구분해서 저장한다. (다른변수들은 그렇지 않음) 이는 강화학습이 MDP(마코프체인+행동+보상)구조를 따르게 때문에 생기는 고유한 특징이다. -- 이론적이 이유\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c64cfab-e57e-4889-8c2b-093c2291f17e",
   "metadata": {},
   "source": [
    "`-` 환경과 에이전트의 상호작용 이해를 위한 다이어그램: \n",
    "\n",
    "- <https://claude.ai/public/artifacts/7fad72b5-0946-47bd-a6cd-b33b21856590> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9d509-233a-4086-9f11-457b60555ac0",
   "metadata": {},
   "source": [
    "# 4. GridWorld 환경의 이해 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4747fa5a-bbe5-4e4e-b2ff-628a8b2678c2",
   "metadata": {},
   "source": [
    "## A. 데이터 축적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb363e2f-5faa-4a5a-bedd-3a69dd9e7ad7",
   "metadata": {},
   "source": [
    "`-` 랜덤에이전트를 이용해 무작위로 100,000 에피소드를 진행해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7f808719-bb01-49d2-b064-c7d08c4347e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "player = RandomAgent()\n",
    "env = GridWorld()\n",
    "scores = [] \n",
    "score = 0 \n",
    "#\n",
    "for e in range(1,100000):\n",
    "    #---에피소드시작---#\n",
    "    while True:\n",
    "        # step1 -- 액션선택\n",
    "        player.act()\n",
    "        # step2 -- 환경반응 \n",
    "        player.next_state, player.reward, player.terminated = env.step(player.action)\n",
    "        # step3 -- 경험기록 & 학습 \n",
    "        player.save_experience()\n",
    "        player.learn()\n",
    "        # step4 --종료 조건 체크 & 후속 처리\n",
    "        if env.terminated:\n",
    "            score = score + player.reward\n",
    "            scores.append(score)\n",
    "            score = 0 \n",
    "            player.state = env.reset() \n",
    "            break\n",
    "        else: \n",
    "            score = score + player.reward            \n",
    "            player.state = player.next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405d6f5-1807-4189-a4f4-e1d200de4d49",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "\n",
    "***강의노트 수정 2025-06-12***\n",
    "\n",
    "노규호학생의 도움으로 예전강의의 오류를 발견하여 수정하였습니다. \n",
    "\n",
    "```Python\n",
    "# 수정전\n",
    "...\n",
    "        if env.terminated:\n",
    "            ...\n",
    "        else: \n",
    "            score = score + player.reward\n",
    "            scores.append(score)            \n",
    "            player.state = player.next_state\n",
    "            \n",
    "# 수정후\n",
    "        if env.terminated:\n",
    "            ...\n",
    "        else: \n",
    "            score = score + player.reward\n",
    "#            scores.append(score)            ### <--- 여기를 주석처리해야함!! \n",
    "            player.state = player.next_state\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ef29fdf9-b027-4d88-8bd7-1e5c6d3b08d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325268"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player.n_experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82c761-757d-49b5-9e6e-2f953a246553",
   "metadata": {},
   "source": [
    "## B. 첫번째 `q_table`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4f33f-53f4-47d5-805a-ce8af721388f",
   "metadata": {},
   "source": [
    "`-` 밴딧게임에서는 $q(a)$ 를 정의했었음. \n",
    "\n",
    "- $q(0) = 1$\n",
    "- $q(1) = 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f13669-e0ce-4c5e-a7bc-b12fc44b48a3",
   "metadata": {},
   "source": [
    "`-` 여기에서는 $q(s_1,s_2,a)$를 정의해야함! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd16fe74-006c-4e6b-99ea-1eb13742a409",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "직관적으로 아래의 그림이 떠오름 \n",
    "![](https://github.com/guebin/DL2025/blob/main/posts/14wk-1-fig1.png?raw=true)\n",
    "\n",
    "그림에 대응하는 $q(s_1,s_2,a)$의 값은 아래와 같음\n",
    "\n",
    ":::{.panel-tabset}\n",
    "\n",
    "### $a=0$\n",
    "\n",
    "$a=0 \\Leftrightarrow \\text{\\tt action=right}$\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "q(0,0,0) & q(0,1,0) & q(0,2,0) & q(0,3,0) \\\\ \n",
    "q(1,0,0) & q(1,1,0) & q(1,2,0) & q(1,3,0) \\\\ \n",
    "q(2,0,0) & q(2,1,0) & q(2,2,0) & q(2,3,0) \\\\ \n",
    "q(3,0,0) & q(3,1,0) &q(3,2,0) & q(3,3,0) \\\\ \n",
    "\\end{bmatrix} =  \\begin{bmatrix}\n",
    "-1 & -1 & -1 & -10 \\\\ \n",
    "-1 & -1 & -1 & -10 \\\\ \n",
    "-1 & -1 & -1 & -10 \\\\ \n",
    "-1 & -1 & 100 &  \\text{-} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### $a=1$\n",
    "\n",
    "$a=1 \\Leftrightarrow \\text{\\tt action=left}$\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "q(0,0,1) & q(0,1,1) & q(0,2,1) & q(0,3,1) \\\\ \n",
    "q(1,0,1) & q(1,1,1) & q(1,2,1) & q(1,3,1) \\\\ \n",
    "q(2,0,1) & q(2,1,1) & q(2,2,1) & q(2,3,1) \\\\ \n",
    "q(3,0,1) & q(3,1,1) &q(3,2,1) & q(3,3,1) \\\\ \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "-10 & -1 & -1 & -1 \\\\ \n",
    "-10& -1 & -1 & -1 \\\\ \n",
    "-10 & -1 & -1 & -1 \\\\ \n",
    "-10 & -1 & -1 &  \\text{-} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### $a=2$\n",
    "\n",
    "$a=2 \\Leftrightarrow \\text{\\tt action=down}$\n",
    "\n",
    "$$  \\begin{bmatrix}\n",
    "q(0,0,2) & q(0,1,2) & q(0,2,2) & q(0,3,2) \\\\ \n",
    "q(1,0,2) & q(1,1,2) & q(1,2,2) & q(1,3,2) \\\\ \n",
    "q(2,0,2) & q(2,1,2) & q(2,2,2) & q(2,3,2) \\\\ \n",
    "q(3,0,2) & q(3,1,2) &q(3,2,2) & q(3,3,2) \\\\ \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "-1 & -1 & -1 & -1 \\\\ \n",
    "-1& -1 & -1 & -1 \\\\ \n",
    "-1 & -1 & -1 & 100\\\\ \n",
    "-10 & -10 & -10 &  \\text{-} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "### $a=3$\n",
    "\n",
    "$a=3 \\Leftrightarrow \\text{\\tt action=up}$\n",
    "\n",
    "$$  \\begin{bmatrix}\n",
    "q(0,0,3) & q(0,1,3) & q(0,2,3) & q(0,3,3) \\\\ \n",
    "q(1,0,3) & q(1,1,3) & q(1,2,3) & q(1,3,3) \\\\ \n",
    "q(2,0,3) & q(2,1,3) & q(2,2,3) & q(2,3,3) \\\\ \n",
    "q(3,0,3) & q(3,1,3) &q(3,2,3) & q(3,3,3) \\\\ \n",
    "\\end{bmatrix} =\\begin{bmatrix}\n",
    "-10 & -10 & -10 & -10\\\\ \n",
    "-1& -1 & -1 & -1 \\\\ \n",
    "-1 & -1 & -1 & -1 \\\\ \n",
    "-1 & -1 & -1 &  \\text{-} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    ":::\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83700793-9da0-4853-ad28-2c18a9839b41",
   "metadata": {},
   "source": [
    "`-` 데이터를 바탕으로 $q(s_1,s_2,a)$를 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dc84ffd7-45da-47ae-94d7-16347398f1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0]), 0, -1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player.states[0], player.actions[0], player.rewards[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2f78c883-feca-45e1-bbdc-166cf754b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((4,4,4))\n",
    "count = np.zeros((4,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "750322b8-bb46-4ac7-87fc-c54f1d3ef782",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory =  zip(player.states, player.actions, player.rewards)\n",
    "for (s1,s2), a, r in memory:\n",
    "    q_table[s1,s2,a] = q_table[s1,s2,a] + r\n",
    "    count[s1,s2,a] = count[s1,s2,a] + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bd64bf9d-eebe-4cc0-abc6-ba4bb934d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count[count==0] = 0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e035d24e-a05a-45c3-9692-5c20719e7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = q_table / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e3c4df1d-8464-4a20-afc0-15d2e10a65a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ -1.,  -1.,  -1., -10.],\n",
       "        [ -1.,  -1.,  -1., -10.],\n",
       "        [ -1.,  -1.,  -1., -10.],\n",
       "        [ -1.,  -1., 100.,   0.]]),\n",
       " array([[-10.,  -1.,  -1.,  -1.],\n",
       "        [-10.,  -1.,  -1.,  -1.],\n",
       "        [-10.,  -1.,  -1.,  -1.],\n",
       "        [-10.,  -1.,  -1.,   0.]]),\n",
       " array([[ -1.,  -1.,  -1.,  -1.],\n",
       "        [ -1.,  -1.,  -1.,  -1.],\n",
       "        [ -1.,  -1.,  -1., 100.],\n",
       "        [-10., -10., -10.,   0.]]),\n",
       " array([[-10., -10., -10., -10.],\n",
       "        [ -1.,  -1.,  -1.,  -1.],\n",
       "        [ -1.,  -1.,  -1.,  -1.],\n",
       "        [ -1.,  -1.,  -1.,   0.]]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[...,0], q_table[...,1], q_table[...,2], q_table[...,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e8ebf-cafc-4575-906c-3d73a9465a1e",
   "metadata": {},
   "source": [
    "`-` count를 사용하지 않는 방법은 없을까? -- 테크닉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "323a393f-c8b0-47d0-872c-3e46f37ed682",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((4,4,4))\n",
    "memory =  zip(player.states, player.actions, player.rewards)\n",
    "for (s1,s2), a, r in memory:\n",
    "    qhat = q_table[s1,s2,a] # 내가 생각했던갓\n",
    "    q = r # 실제값\n",
    "    diff = q-qhat # 차이\n",
    "    q_table[s1,s2,a] = q_table[s1,s2,a]  + 0.01*diff# update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5b395794-2199-42c4-84ae-3c55448b0c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -1.  , -10.  ,  -1.  , -10.  ],\n",
       "        [ -1.  ,  -1.  ,  -1.  , -10.  ],\n",
       "        [ -1.  ,  -1.  ,  -1.  , -10.  ],\n",
       "        [-10.  ,  -1.  ,  -1.  , -10.  ]],\n",
       "\n",
       "       [[ -1.  , -10.  ,  -1.  ,  -1.  ],\n",
       "        [ -1.  ,  -1.  ,  -1.  ,  -1.  ],\n",
       "        [ -1.  ,  -1.  ,  -1.  ,  -1.  ],\n",
       "        [-10.  ,  -1.  ,  -1.  ,  -1.  ]],\n",
       "\n",
       "       [[ -1.  , -10.  ,  -1.  ,  -1.  ],\n",
       "        [ -1.  ,  -1.  ,  -1.  ,  -1.  ],\n",
       "        [ -1.  ,  -1.  ,  -1.  ,  -1.  ],\n",
       "        [-10.  ,  -1.  ,  99.99,  -1.  ]],\n",
       "\n",
       "       [[ -1.  , -10.  , -10.  ,  -1.  ],\n",
       "        [ -1.  ,  -1.  , -10.  ,  -1.  ],\n",
       "        [ 99.99,  -1.  , -10.  ,  -1.  ],\n",
       "        [  0.  ,   0.  ,   0.  ,   0.  ]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f539f-1f55-40d3-8e23-fd1ac256953f",
   "metadata": {},
   "source": [
    "## C. 첫번째 `q_table`보다 나은 것?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f0335-3958-43b2-bfee-43eaede1cab2",
   "metadata": {},
   "source": [
    "`-` 첫번째 `q_table`을 알고있다고 가정하자. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74bee9ac-afc0-4970-b6d9-f4ec23ea0bef",
   "metadata": {},
   "source": [
    "![](https://github.com/guebin/DL2025/blob/main/posts/14wk-1-fig1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f93fe-649f-43ab-8def-35356e0b50e6",
   "metadata": {},
   "source": [
    "`-` 정책시각화 (합리적인 행동) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e51f9ad5-16fe-4d27-a94f-6342de087b2b",
   "metadata": {},
   "source": [
    "![](https://github.com/guebin/DL2025/blob/main/posts/14wk-1-fig2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0524ca-d047-461b-bf3d-eedcf888ca5e",
   "metadata": {},
   "source": [
    "`-` 이게 최선의 정책일까?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
