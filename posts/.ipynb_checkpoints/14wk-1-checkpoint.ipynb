{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7a9ee665-7172-4c03-97f3-96393b519497",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"14wk-1: (강화학습) -- 4x4 Grid World 환경의 이해\"\n",
    "author: \"최규빈\"\n",
    "date: \"06/02/2025\"\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860568c9-90af-4b0e-afc7-6040ce1592e7",
   "metadata": {
    "id": "e67ab8e0"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/13wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782deb0-080f-45f8-b122-c6eb7285ed47",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 강의영상 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc846c-6bc5-4a3c-9cd5-a2d549f39c54",
   "metadata": {
    "tags": []
   },
   "source": [
    "{{<video https://youtu.be/playlist?list=PLQqh36zP38-xRfokRN58uC0Mr4NGrJiWX&si=PhVcbmFUN0x3TLfN >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec70d12-2d53-494e-90a5-bc3b07727127",
   "metadata": {},
   "source": [
    "# 2. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d667404-aced-49db-b8c1-bb4e41eba4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "#---#\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9c3d76-670a-4283-997c-cd4f5d373781",
   "metadata": {},
   "source": [
    "# 3. 지난시간 코드 복습 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ad7cd-f74f-4d0a-9f48-41b4b234e676",
   "metadata": {},
   "source": [
    "`-` 클래스 -- 수정사항없음 // 편의상 print하는 코드만 주석처리함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ea8f17-ff1a-47a6-b30e-9c2e0ca9fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.a2d = {\n",
    "            0: np.array([0,1]), # →\n",
    "            1: np.array([0,-1]), # ←  \n",
    "            2: np.array([1,0]),  # ↓\n",
    "            3: np.array([-1,0])  # ↑\n",
    "        }\n",
    "        self.state_space = gym.spaces.MultiDiscrete([4,4])\n",
    "        self.state = np.array([0,0])\n",
    "        self.reward = None\n",
    "        self.terminated = False\n",
    "    def step(self,action):\n",
    "        self.state = self.state + self.a2d[action]\n",
    "        s1,s2 = self.state\n",
    "        if (s1==3) and (s2==3):\n",
    "            self.reward = 100 \n",
    "            self.terminated = True\n",
    "        elif self.state in self.state_space:\n",
    "            self.reward = -1 \n",
    "            self.terminated = False\n",
    "        else:\n",
    "            self.reward = -10\n",
    "            self.terminated = True\n",
    "        # print(\n",
    "        #     f\"action = {action}\\t\"\n",
    "        #     f\"state = {self.state - self.a2d[action]} -> {self.state}\\t\"\n",
    "        #     f\"reward = {self.reward}\\t\"\n",
    "        #     f\"termiated = {self.terminated}\"\n",
    "        # )            \n",
    "        return self.state, self.reward, self.terminated\n",
    "    def reset(self):\n",
    "        self.state = np.array([0,0])\n",
    "        self.terminated = False\n",
    "        return self.state\n",
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.state = None \n",
    "        self.action = None \n",
    "        self.reward = None \n",
    "        self.next_state = None\n",
    "        self.terminated = None\n",
    "        #---#\n",
    "        self.states = collections.deque(maxlen=500)\n",
    "        self.actions = collections.deque(maxlen=500)\n",
    "        self.rewards = collections.deque(maxlen=500)\n",
    "        self.next_states = collections.deque(maxlen=500)\n",
    "        self.terminations = collections.deque(maxlen=500)\n",
    "        #---#\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.n_experience = 0\n",
    "    def act(self):\n",
    "        self.action = self.action_space.sample()\n",
    "    def save_experience(self):\n",
    "        self.states.append(self.state)\n",
    "        self.actions.append(self.action)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.next_states.append(self.next_state)\n",
    "        self.terminations.append(self.terminated)\n",
    "        self.n_experience = self.n_experience + 1\n",
    "    def learn(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d55f9-775e-4645-b06b-1efd64e23572",
   "metadata": {},
   "source": [
    "`-` 메인코드 -- 구조를 수정함 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1782b1-b299-4364-b358-374f404f20a7",
   "metadata": {},
   "source": [
    "```Python\n",
    "player = RandomAgent()\n",
    "env = GridWorld()\n",
    "scores = [] \n",
    "score = 0 \n",
    "#\n",
    "for e in range(1,100):\n",
    "    #---에피소드시작---#\n",
    "    while True:\n",
    "        # step1 -- 액션선택\n",
    "        player.act()\n",
    "        # step2 -- 환경반응 \n",
    "        player.next_state, player.reward, player.terminated = env.step(player.action)\n",
    "        # step3 -- 경험기록 & 학습 \n",
    "        player.save_experience()\n",
    "        player.learn()\n",
    "        # step4 --종료 조건 체크 & 후속 처리\n",
    "        if env.terminated:\n",
    "            score = score + player.reward\n",
    "            scores.append(score)\n",
    "            score = 0 \n",
    "            player.state = env.reset() \n",
    "            print(f\"---에피소드{e}종료---\")\n",
    "            break\n",
    "        else: \n",
    "            score = score + player.reward\n",
    "            scores.append(score)            \n",
    "            player.state = player.next_state\n",
    "    #---에피소드끝---#\n",
    "    if scores[-1] > 0:\n",
    "        break\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5b849-087c-4b3b-be12-9f3e44d42749",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "마음에 들지 않지만 꼭 외워야 하는것 \n",
    "\n",
    "1. `env.step`은 항상 next_state, reward, terminated, truncated, info 를 리턴한다. -- 짐나지엄 라이브러리 규격때문\n",
    "2. `env.reset`은 환경을 초기화할 뿐만 아니라, state, info를 반환하는 기능도 있다.  -- 짐나지엄 라이브러리 규격때문\n",
    "3. `player`는 항상 `state`와 `next_state`를 구분해서 저장한다. (다른변수들은 그렇지 않음) 이는 강화학습이 MDP(마코프체인+행동+보상)구조를 따르게 때문에 생기는 고유한 특징이다. -- 이론적이 이유\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c64cfab-e57e-4889-8c2b-093c2291f17e",
   "metadata": {},
   "source": [
    "`-` 환경과 에이전트의 상호작용 이해를 위한 다이어그램: <https://claude.ai/public/artifacts/7fad72b5-0946-47bd-a6cd-b33b21856590> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9d509-233a-4086-9f11-457b60555ac0",
   "metadata": {},
   "source": [
    "# 3. 환경의 이해 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4747fa5a-bbe5-4e4e-b2ff-628a8b2678c2",
   "metadata": {},
   "source": [
    "## A. 데이터 축적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb363e2f-5faa-4a5a-bedd-3a69dd9e7ad7",
   "metadata": {},
   "source": [
    "`-` 랜덤에이전트를 이용해 무작위로 10000판을 진행해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f808719-bb01-49d2-b064-c7d08c4347e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "player = RandomAgent()\n",
    "env = GridWorld()\n",
    "scores = [] \n",
    "score = 0 \n",
    "#\n",
    "for e in range(1,10000):\n",
    "    #---에피소드시작---#\n",
    "    while True:\n",
    "        # step1 -- 액션선택\n",
    "        player.act()\n",
    "        # step2 -- 환경반응 \n",
    "        player.next_state, player.reward, player.terminated = env.step(player.action)\n",
    "        # step3 -- 경험기록 & 학습 \n",
    "        player.save_experience()\n",
    "        player.learn()\n",
    "        # step4 --종료 조건 체크 & 후속 처리\n",
    "        if env.terminated:\n",
    "            score = score + player.reward\n",
    "            scores.append(score)\n",
    "            score = 0 \n",
    "            player.state = env.reset() \n",
    "            break\n",
    "        else: \n",
    "            score = score + player.reward\n",
    "            scores.append(score)            \n",
    "            player.state = player.next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef29fdf9-b027-4d88-8bd7-1e5c6d3b08d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32195"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player.n_experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82c761-757d-49b5-9e6e-2f953a246553",
   "metadata": {},
   "source": [
    "## B. 첫번째 `q_table`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4f33f-53f4-47d5-805a-ce8af721388f",
   "metadata": {},
   "source": [
    "`-` 밴딧게임에서는 $Q(a)$ 를 정의했었음. \n",
    "\n",
    "- $Q(0) = 1$\n",
    "- $Q(1) = 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f13669-e0ce-4c5e-a7bc-b12fc44b48a3",
   "metadata": {},
   "source": [
    "`-` 여기에서는 $Q(s_1,s_2,a)$를 정의해야함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc84ffd7-45da-47ae-94d7-16347398f1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0]), 3, -1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player.states[0], player.actions[0], player.rewards[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a947556-5da0-4866-b4bc-5a6d86d4e3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750322b8-bb46-4ac7-87fc-c54f1d3ef782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
