{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e1ae6-71b1-4078-8c46-b43c7162710e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2fc065-61bd-4824-9ce5-baf1eaabc373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9ae58c-5b5a-4167-be2b-245da5017e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!apt-get install swig\n",
    "#!pip install gymnasium[box2d]\n",
    "import gymnasium as gym\n",
    "#--#\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import IPython\n",
    "#--#\n",
    "import collections\n",
    "import random\n",
    "#--#\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df7ccd3-d6ef-4e43-8ef4-26c2bfe66a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show(ims,jump=10):\n",
    "    ims = ims[::jump]\n",
    "    fig = plt.Figure()\n",
    "    ax = fig.subplots()\n",
    "    def update(i):\n",
    "        ax.imshow(ims[i])\n",
    "    ani = FuncAnimation(fig,update,frames=len(ims))\n",
    "    display(IPython.display.HTML(ani.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a9fa2b-effa-488f-966b-3308d6afebdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AgentRandom: \n",
    "    def __init__(self):\n",
    "        #--# define spaces \n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        #\n",
    "        self.state =  None    ## 길이가 8인 np.array        \n",
    "        self.action = None            ## int, 0,1,2,3 중 하나\n",
    "        self.reward = None            ## float  \n",
    "        self.next_state =  None       ## np.array\n",
    "        self.terminated = None        ## bool \n",
    "        #\n",
    "        self.buffer_size = 5000\n",
    "        self.states = collections.deque(maxlen=self.buffer_size) # 원소는 텐서         \n",
    "        self.actions = collections.deque(maxlen=self.buffer_size) # 원소는 텐서 \n",
    "        self.rewards = collections.deque(maxlen=self.buffer_size) # 원소는 텐서\n",
    "        self.next_states = collections.deque(maxlen=self.buffer_size) # 원소는 텐서 \n",
    "        self.terminations = collections.deque(maxlen=self.buffer_size) # 원소는 텐서 \n",
    "        #\n",
    "        self.n_experiences = 0 \n",
    "    def act(self):\n",
    "        self.action = self.action_space.sample()\n",
    "    def learn(self):\n",
    "        pass \n",
    "    def save_experience(self):\n",
    "        self.states.append(torch.tensor(self.state))\n",
    "        self.actions.append(torch.tensor(self.action))\n",
    "        self.rewards.append(torch.tensor(self.reward))\n",
    "        self.next_states.append(torch.tensor(self.next_state))\n",
    "        self.terminations.append(torch.tensor(self.terminated))           \n",
    "        #--#\n",
    "        self.n_experiences = self.n_experiences + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4eb4f9-d094-4d09-8943-85d15b6dc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(AgentRandom):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 0 \n",
    "        self.q_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8,256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256,128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128,64),\n",
    "            torch.nn.ReLU(),    \n",
    "            torch.nn.Linear(64,4)\n",
    "        )\n",
    "        self.optimizr = torch.optim.Adam(self.q_net.parameters(),lr=0.0001)\n",
    "        self.batch_size = 64\n",
    "    def act(self):\n",
    "        if random.random() < self.eps: \n",
    "            self.action = self.action_space.sample()\n",
    "        else: \n",
    "            s = torch.tensor(self.state)\n",
    "            self.action = self.q_net(s).argmax().item() \n",
    "    def learn(self):\n",
    "        if self.n_experiences < self.batch_size:\n",
    "            pass \n",
    "        else: \n",
    "            for epoc in range(1):\n",
    "                memory = list(zip(\n",
    "                    self.states,\n",
    "                    self.actions,\n",
    "                    self.rewards,\n",
    "                    self.next_states,\n",
    "                    self.terminations\n",
    "                ))\n",
    "                minibatch = random.sample(memory,self.batch_size)\n",
    "                ## step 1~2 \n",
    "                loss = 0 \n",
    "                for s,a,r,ss,tmd in minibatch:\n",
    "                    # step1: q_hat \n",
    "                    q_hat = self.q_net(s)[a]        \n",
    "                    # step2: loss를 계산한다. \n",
    "                    if self.terminated:\n",
    "                        q = r\n",
    "                    else:\n",
    "                        future = self.q_net(ss).max().data\n",
    "                        q = r + 0.99 * future\n",
    "                    loss = loss + (q_hat-q)**2 \n",
    "                loss = loss / self.batch_size \n",
    "                # step3 \n",
    "                loss.backward()\n",
    "                # step4 \n",
    "                self.optimizr.step()\n",
    "                self.optimizr.zero_grad() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57f20666-6207-41c5-a600-bb62a89d1e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\",render_mode = 'rgb_array')\n",
    "player_dummy = Agent() \n",
    "#player_dummy.q_net = player.q_net # 비법전수 \n",
    "player_dummy.q_net.load_state_dict(torch.load(\"2025q_net_600.pth\"))\n",
    "player_dummy.state, _ = env.reset()\n",
    "score = 0 \n",
    "ims = [] \n",
    "ims.append(env.render())\n",
    "for t in range(1001):\n",
    "    player_dummy.act() \n",
    "    player_dummy.next_state, player_dummy.reward, player_dummy.terminated, player_dummy.truncated, _  = env.step(player_dummy.action)\n",
    "    score = score + player_dummy.reward\n",
    "    ims.append(env.render())\n",
    "    player_dummy.state = player_dummy.next_state\n",
    "    if player_dummy.terminated or player_dummy.truncated: \n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940561d1-2d28-4b35-9fda-bc322ca4c5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show(ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438dbe8-beef-4b16-a2a1-4ac1c25cdb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
