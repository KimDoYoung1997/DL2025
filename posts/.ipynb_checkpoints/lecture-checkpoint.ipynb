{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a38c22cf-74ca-41d2-95ba-b973a0263535",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"lecture\"\n",
    "author: \"최규빈\"\n",
    "date: \"05/19/2025\"\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b27a7a-19b0-4cf3-bcf4-8db99f55036d",
   "metadata": {
    "id": "e67ab8e0"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/09wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34257653-1a9b-4b05-8a0f-fa267a77c995",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 강의영상 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "99d156c1-b7cd-4735-b27b-ed4bbe961181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# {{<video https://youtu.be/playlist?list=PLQqh36zP38-zEjn2m8H8hMCHsQK8udE27&si=Sy-lnw4Kq56SRggu >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70183d4-475c-4d1c-beb3-3b32aa565142",
   "metadata": {},
   "source": [
    "# 2. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e5af06a9-81d1-446e-baca-bdb9b5e722e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0708b5fa-4ed9-48c4-a463-3cf3402b303e",
   "metadata": {},
   "source": [
    "# 3. 강화학습 Intro "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7c4cae-4d92-4b12-b9af-2c20a2dc4ac1",
   "metadata": {},
   "source": [
    "`-` 강화학습(대충설명): 어떠한 \"(게임)환경\"이 있을때 거기서 \"뭘 할지\"를 학습하는 과업"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0be0b7a3-240d-48f0-88b2-5390b19802a5",
   "metadata": {},
   "source": [
    "![그림1: 셔튼(@sutton1998reinforcement)의 교재에서 발췌한 그림, 되게 유명한 그림이에요](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda26e7-ed3c-4fd8-9ab4-af00c4bb8f4d",
   "metadata": {},
   "source": [
    "`-` 딥마인드: breakout $\\to$ 알파고 \n",
    "\n",
    "- <https://www.youtube.com/watch?v=TmPfTpjtdgg>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a9f3176-0158-46ed-bec5-435ef28aa879",
   "metadata": {},
   "source": [
    "![그림2: 벽돌깨기](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig2.png?raw=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad48af23-475e-45b5-b90b-738190008592",
   "metadata": {},
   "source": [
    "`-` 강화학습에서 \"강화\"는 뭘 강화한다는것일까? \n",
    "\n",
    "- <https://k9connoisseur.com/blogs/news/positive-reinforcement-dog-training> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411206af-ca04-4739-be37-88c014fb6dad",
   "metadata": {},
   "source": [
    "`-` 강화학습 미래? (이거 잘하면 먹고 살 수 있을까?) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062957eb-641e-4801-b66b-d04e8b93cd15",
   "metadata": {},
   "source": [
    "# 4. Bandit 게임 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e49c0-5f27-4629-91bc-5665bf6194c1",
   "metadata": {},
   "source": [
    "`-` 문제설명: 두 개의 버튼이 있다. `버튼0`을 누르면 1의 보상을, `버튼1`을 누르면 10의 보상을 준다고 가정 \n",
    "\n",
    "- Agent: 버튼0을 누르거나,버튼1을 누르는 존재 \n",
    "- Env: Agent의 Action을 바탕으로 Reward를 주는 존재\n",
    "\n",
    "> 주의: 이 문제 상황에서 state는 없음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08bbeb-fb13-446d-88c9-5a374fcac7eb",
   "metadata": {},
   "source": [
    "`-` 생성형AI로 위의 상황을 설명한것 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a762101-f292-47eb-8ecb-82dee6c8877a",
   "metadata": {},
   "source": [
    "::: {layout-ncol=3}\n",
    "![(a) 챗지피티로 생성한그림](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gpt.png?raw=true) \n",
    "\n",
    "![(b) 제미나이로 생성한 그림](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gemini.png?raw=true)\n",
    "\n",
    "![(c)  퍼플렉시티로 생성한 그림](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-perplexity.png?raw=true)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe917ee6-6754-4383-a6fa-36b6aa09357a",
   "metadata": {},
   "source": [
    "- 클로드로 생성: <https://claude.ai/public/artifacts/1f52fcb2-ef08-4af1-8cf8-4a497d7bcc5f>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd67038-356e-4e0d-a1ed-7066e7914084",
   "metadata": {},
   "source": [
    "`-` 게임진행양상\n",
    "\n",
    "- 처음에는 아는게 없음. 일단 \"아무거나\" 눌러보자. (\"에이전트가 랜덤액션을 한다\" 고 표현함 )\n",
    "- 한 20번 정도 눌러보면서 결과를 관찰함 (\"에이전트가 경험을 축적한다\"고 표현함) \n",
    "- 버튼0을 누를때는 1점, 버튼1을 누를때는 10점을 준다는 사실을 깨달음. (\"에이전트가 환경을 이해했다\"고 표현함)\n",
    "- 버튼1을 누르는게 나한테 이득이 라는 사실을 깨달음. (\"에이전트가 최적의 정책을 학습했다\" 고 표현함)\n",
    "- 이제부터 무조건 버튼1만 누름 $\\to$ 게임 클리어 (\"강화학습 성공\"이라 표현할 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c57e6-030e-4e77-b762-214446d74196",
   "metadata": {},
   "source": [
    "`-` 어떻게 버튼1을 누르는게 이득이라는 사실을 아는거지? $\\to$ 아래와 같은 테이블을 만들면 된다. (`q_table`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f832cbd-0720-49b6-bd51-c1c26baa2e8a",
   "metadata": {},
   "source": [
    "|| Action0 | Action1 | \n",
    "|:-:|:-:|:-:|\n",
    "|State0 | mean(Reward \\| State0, Action0) |mean(Reward \\| State0, Action1)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e8fa90-e56c-4970-8589-ced0202cc22b",
   "metadata": {},
   "source": [
    "# 5. 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a1575-a5eb-4fe1-8e10-a696d53ee852",
   "metadata": {},
   "source": [
    "## A. 대충 개념만 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "e11541cc-a81d-437f-94b6-5cfd1f6b7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [0,1] \n",
    "actions_deque = collections.deque(maxlen=500)\n",
    "rewards_deque =  collections.deque(maxlen=500)\n",
    "#---#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "6881c709-797e-474e-a0e2-f285060118ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    action = np.random.choice(action_space)\n",
    "    if action == 1:\n",
    "        reward = 10 \n",
    "    else:\n",
    "        reward = 1\n",
    "    actions_deque.append(action)\n",
    "    rewards_deque.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "c07fb7f3-adeb-43f7-881f-1734ee3f3228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([0, 1, 0, 0, 0, 1, 0, 1, 1, 0], maxlen=500)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "24dd511c-51d7-42dc-8793-9e9f8135fed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([1, 10, 1, 1, 1, 10, 1, 10, 10, 1], maxlen=500)"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "7fd97f5a-e5b0-4c0a-b301-86a89670ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_numpy = np.array(actions_deque)\n",
    "rewards_numpy = np.array(rewards_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "eb4ef9c6-3218-46bd-831e-bc414b320c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., 10.])"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q0 = rewards_numpy[actions_numpy == 0].mean()\n",
    "q1 = rewards_numpy[actions_numpy == 1].mean()\n",
    "q_table = np.array([q0,q1])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "d69e0248-6f60-4fbf-adda-54529428166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = q_table.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "8c4a5aa4-a7de-494d-8a53-b0c765f1f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    action = q_table.argmax()\n",
    "    if action == 1:\n",
    "        reward = 10 \n",
    "    else:\n",
    "        reward = 1\n",
    "    actions_deque.append(action)\n",
    "    rewards_deque.append(reward)\n",
    "    actions_numpy = np.array(actions_deque)\n",
    "    rewards_numpy = np.array(rewards_deque)    \n",
    "    q0 = rewards_numpy[actions_numpy == 0].mean()\n",
    "    q1 = rewards_numpy[actions_numpy == 1].mean()\n",
    "    q_table = np.array([q0,q1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "e4d02dd3-15b4-4b43-9a7c-81c89ec899a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "da08ea5a-c0ad-4a74-af5e-3f42314ede05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 10,  1,  1,  1, 10,  1, 10, 10,  1, 10, 10, 10, 10, 10])"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6065a1-fb75-4350-9297-bde6c82c4e0a",
   "metadata": {},
   "source": [
    "## B. 클래스를 이용한 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "95e30c5e-5415-4820-8e70-f193c98a2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self):\n",
    "        self.reward = None \n",
    "    def step(self,action):\n",
    "        if action == 0:\n",
    "            self.reward = 1\n",
    "        else: \n",
    "            self.reward = 10 \n",
    "        return self.reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "6a3275de-bc5c-490c-8675-7fa0e20e68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Bandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "0c914273-8f9c-4295-8079-b4e0ce4aab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    def act(self):\n",
    "        # 만약에 경험이 20보다 작음 --> 랜덤액션 \n",
    "        # 경험이 20보다 크면 --> action = q_tabel.argmax()\n",
    "        pass \n",
    "    def save_experience(self):\n",
    "        # 데이터 저장 \n",
    "        pass \n",
    "    def learn(self):\n",
    "        # q_table 을 업데이트하는 과정 \n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
