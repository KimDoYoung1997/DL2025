{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7536d01f-6566-433a-8e13-b3189002341b",
   "metadata": {
    "id": "87b5cded-346b-4915-acf5-b5ec93a5207d",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"07wk-2: (합성곱신경망) -- CNN 핵심레이어, CNN 학습원리\"\n",
    "author: \"최규빈\"\n",
    "date: \"04/21/2025\"\n",
    "draft: true\n",
    "freeze: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd065092-e523-4288-af77-23ad3e9e1690",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/07wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19edb483-e3e0-4397-b17e-87f4abb2b919",
   "metadata": {
    "id": "4d47a7c9",
    "tags": []
   },
   "source": [
    "# 1. 강의영상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9fe81f-4d33-4f4d-82b3-3ea7cfd2797e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# {{<video https://youtu.be/playlist?list=PLQqh36zP38-wZj26ZHPIZ6kvcrPiL92q3&si=UF3qTgEXmUqnV4e->}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c1ccd-15c2-4981-ba73-2edb6dba8dac",
   "metadata": {},
   "source": [
    "# 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd228b9-8b1b-458d-a7ce-148206b6bf2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce5fed71-61cf-4a49-916d-3256b57125df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4.5, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52caf0-ec1b-4acd-961a-53d75e2dcb59",
   "metadata": {},
   "source": [
    "# 3. CNN 핵심레이어 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15bbe0-20bb-4bd2-a540-3ffc150fca3f",
   "metadata": {},
   "source": [
    "## A. `torch.nn.ReLU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d18018-2908-4b62-b1d2-0b68da1d3e1d",
   "metadata": {},
   "source": [
    "## B. `torch.nn.MaxPool2d`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe1524-c8f1-4e5f-834e-03fe2c605965",
   "metadata": {},
   "source": [
    "## C. `torch.nn.Conv2d`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f587a-5b2f-4acd-9aa2-f4584919b176",
   "metadata": {},
   "source": [
    "**(예시1) 연산방법, stride=2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418e094b-6fbb-4e4b-95c5-77c35d8f0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(1,1,4,4)\n",
    "conv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2,stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24dd62d8-6f6a-4358-92d9-edc305ed184e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.7527, 0.4291, 0.5222, 0.7240],\n",
       "          [0.8182, 0.3177, 0.2914, 0.2979],\n",
       "          [0.5819, 0.4201, 0.1881, 0.9395],\n",
       "          [0.7151, 0.2013, 0.1069, 0.2511]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84eb6571-3ad4-4491-93c8-be63888758c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1281, 0.0595],\n",
       "          [0.1877, 0.0775]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e698e4-5ba0-4a56-9dc5-2870f5e2ce9d",
   "metadata": {},
   "source": [
    "??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c34ce339-f5e8-4728-b723-476c5c5cb277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.2970, -0.3377],\n",
       "           [ 0.0756, -0.1169]]]]),\n",
       " tensor([0.4719]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.data, conv.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ac01d2-71c3-49cc-a39f-ce072f61ff83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1281]),\n",
       " tensor([[[[0.1281, 0.0595],\n",
       "           [0.1877, 0.0775]]]], grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img[:,  :,  :2,  :2] * conv.weight.data).sum()+conv.bias.data, conv(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1a0311e-689e-43be-a417-c941a8b378fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0595]),\n",
       " tensor([[[[0.1281, 0.0595],\n",
       "           [0.1877, 0.0775]]]], grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img[:,  :,  :2,  2:] * conv.weight.data).sum()+conv.bias.data, conv(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e175c600-977d-44cb-9e39-0af91a980e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0595]),\n",
       " tensor([[[[0.1281, 0.0595],\n",
       "           [0.1877, 0.0775]]]], grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img[:,  :,  :2,  2:] * conv.weight.data).sum()+conv.bias.data, conv(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a63a3712-b454-4ec8-8fa3-ca6b8010f669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0775]),\n",
       " tensor([[[[0.1281, 0.0595],\n",
       "           [0.1877, 0.0775]]]], grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img[:,  :,  2:,  2:] * conv.weight.data).sum()+conv.bias.data, conv(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbafa710-5d77-4d5b-bcd2-67695039329b",
   "metadata": {},
   "source": [
    "**(예시2) 이동평균**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dfef79c-8034-4360-9d65-762894de1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.arange(1,17).float().reshape(1,1,4,4)\n",
    "conv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2,stride=1,bias=False)\n",
    "conv.weight.data = conv.weight.data*0+1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e64ea61-8201-4890-ba6b-274f2d828452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ad36af0-772e-4f60-9d2c-c6beb4bbff8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.5000,  4.5000,  5.5000],\n",
       "          [ 7.5000,  8.5000,  9.5000],\n",
       "          [11.5000, 12.5000, 13.5000]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c03385-ea7a-40a8-8ab6-504b303ab497",
   "metadata": {},
   "source": [
    "**(예시3) 2개의 이미지**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "770ce1b2-6d33-40af-9ace-4d86ca5ea6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.arange(1,33).float().reshape(2,1,4,4)\n",
    "conv = torch.nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2,stride=1,bias=False)\n",
    "conv.weight.data = conv.weight.data*0+1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eed170a1-f9db-4571-9afd-482752847dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]],\n",
       "\n",
       "\n",
       "        [[[17., 18., 19., 20.],\n",
       "          [21., 22., 23., 24.],\n",
       "          [25., 26., 27., 28.],\n",
       "          [29., 30., 31., 32.]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb84265b-a765-4f4e-968b-be89a284b773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.5000,  4.5000,  5.5000],\n",
       "          [ 7.5000,  8.5000,  9.5000],\n",
       "          [11.5000, 12.5000, 13.5000]]],\n",
       "\n",
       "\n",
       "        [[[19.5000, 20.5000, 21.5000],\n",
       "          [23.5000, 24.5000, 25.5000],\n",
       "          [27.5000, 28.5000, 29.5000]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db3280-e440-4f11-a852-0cb4fccc42c0",
   "metadata": {},
   "source": [
    "**(예시4) 2개의 out_channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f31b61f5-5116-4729-96ca-caff1a258bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.arange(1,33).float().reshape(2,1,4,4)\n",
    "conv = torch.nn.Conv2d(in_channels=1,out_channels=2,kernel_size=2,stride=1,bias=False)\n",
    "conv.weight.data[0] = conv.weight.data[0]*0 + 1/4\n",
    "conv.weight.data[1] = conv.weight.data[1]*0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f983b082-bfa8-40cd-a74c-643fabbe030f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]],\n",
       "\n",
       "\n",
       "        [[[17., 18., 19., 20.],\n",
       "          [21., 22., 23., 24.],\n",
       "          [25., 26., 27., 28.],\n",
       "          [29., 30., 31., 32.]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "161d29ed-c100-413e-9715-f7df683af7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.5000,  4.5000,  5.5000],\n",
       "          [ 7.5000,  8.5000,  9.5000],\n",
       "          [11.5000, 12.5000, 13.5000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[19.5000, 20.5000, 21.5000],\n",
       "          [23.5000, 24.5000, 25.5000],\n",
       "          [27.5000, 28.5000, 29.5000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
