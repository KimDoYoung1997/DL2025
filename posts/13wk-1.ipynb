{
 "cells": [
  {
   "cell_type": "raw",
   "id": "20ee4531-eaa2-4d6d-acb8-4cd9edf06ac9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"13wk-1: (강화학습) -- 강화학습 Intro, Bandit 게임 설명, Bandit 환경 설계 및 풀이\"\n",
    "author: \"최규빈\"\n",
    "date: \"05/28/2025\"\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21686a35-0ef2-4663-971c-f02b8ad79474",
   "metadata": {
    "id": "e67ab8e0"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/DL2025/blob/main/posts/13wk-1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237b5b0-69a8-44e3-b444-58261c5ba538",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 강의영상 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac087b-6c6e-42e8-be17-ad7570ee4ea3",
   "metadata": {
    "tags": []
   },
   "source": [
    "{{<video https://youtu.be/playlist?list=PLQqh36zP38-xkKWgGZTE_11TnXRdBtAW-&si=dMVGdO_bdkObYiLO >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54afc5-3248-4e78-a6cd-e3f397c1bc03",
   "metadata": {},
   "source": [
    "# 2. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e67c2fbc-b914-41de-ba7c-d1ca82bbe3e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7647998-5c41-471b-88a9-a04b022dfeaa",
   "metadata": {},
   "source": [
    "# 3. 강화학습 Intro "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294a5d7-09da-484d-97e9-587b8e4f9628",
   "metadata": {},
   "source": [
    "`-` 강화학습(대충설명): 어떠한 \"(게임)환경\"이 있을때 거기서 \"뭘 할지\"를 학습하는 과업"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6959084-9697-4a84-96ad-6276be7e553e",
   "metadata": {},
   "source": [
    "![그림1: 셔튼(@sutton1998reinforcement)의 교재에서 발췌한 그림, 되게 유명한 그림이에요](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b496219-1194-48a2-9a70-3c2be6311558",
   "metadata": {},
   "source": [
    "`-` 딥마인드: breakout $\\to$ 알파고 \n",
    "\n",
    "- <https://www.youtube.com/watch?v=TmPfTpjtdgg>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38ccaff6-78c7-47ef-afd1-bdb4eb6e83ed",
   "metadata": {},
   "source": [
    "![그림2: 벽돌깨기](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig2.png?raw=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f3d2bd6-146b-4656-99cc-528085d87b81",
   "metadata": {},
   "source": [
    "`-` 강화학습에서 \"강화\"는 뭘 강화한다는것일까? \n",
    "\n",
    "- <https://k9connoisseur.com/blogs/news/positive-reinforcement-dog-training> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd25bb7-2776-423c-8b3b-5fa7cd591621",
   "metadata": {},
   "source": [
    "`-` 강화학습 미래? (이거 잘하면 먹고 살 수 있을까?) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651b5af-9328-4df2-b8ea-604d50528be3",
   "metadata": {},
   "source": [
    "# 4. Bandit 게임 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae247d25-c760-43de-8079-f961f7d93011",
   "metadata": {},
   "source": [
    "`-` 문제설명: 두 개의 버튼이 있다. `버튼0`을 누르면 1의 보상을, `버튼1`을 누르면 10의 보상을 준다고 가정 \n",
    "\n",
    "- Agent: 버튼0을 누르거나,버튼1을 누르는 존재 \n",
    "- Env: Agent의 Action을 바탕으로 Reward를 주는 존재\n",
    "\n",
    "> 주의: 이 문제 상황에서 state는 없음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e481d6-e2f9-41db-be55-d079c84dfb0e",
   "metadata": {},
   "source": [
    "`-` 생성형AI로 위의 상황을 설명한것 (왼쪽부터 챗지피티, 제미나이, 퍼플렉시티의 결과)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f1aee-7a9d-45a5-9469-efeac0fba8b4",
   "metadata": {},
   "source": [
    "::: {layout-ncol=3}\n",
    "![](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gpt.png?raw=true)\n",
    "\n",
    "![](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-gemini.png?raw=true)\n",
    "\n",
    "![](https://github.com/guebin/DL2025/blob/main/posts/13wk-1-fig3-perplexity.png?raw=true)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29634314-350f-4c95-9237-a3b0da263698",
   "metadata": {},
   "source": [
    "- 클로드로 생성: <https://claude.ai/public/artifacts/1f52fcb2-ef08-4af1-8cf8-4a497d7bcc5f>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c58b3-c091-4510-802d-8a97310af06b",
   "metadata": {},
   "source": [
    "`-` 게임진행양상\n",
    "\n",
    "- 처음에는 아는게 없음. 일단 \"아무거나\" 눌러보자. (\"에이전트가 랜덤액션을 한다\" 고 표현함 )\n",
    "- 한 20번 정도 눌러보면서 결과를 관찰함 (\"에이전트가 경험을 축적한다\"고 표현함) \n",
    "- 버튼0을 누를때는 1점, 버튼1을 누를때는 10점을 준다는 사실을 깨달음. (\"에이전트가 환경을 이해했다\"고 표현함)\n",
    "- 버튼1을 누르는게 나한테 이득이 라는 사실을 깨달음. (\"에이전트가 최적의 정책을 학습했다\" 고 표현함)\n",
    "- 이제부터 무조건 버튼1만 누름 $\\to$ 게임 클리어 (\"강화학습 성공\"이라 표현할 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0051f-eefb-4207-ac46-7f95eee8d81b",
   "metadata": {},
   "source": [
    "`-` 어떻게 버튼1을 누르는게 이득이라는 사실을 아는거지? $\\to$ 아래와 같은 테이블을 만들면 된다. (`q_table`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69913a6d-27c5-4760-81ed-aa29f6d22744",
   "metadata": {},
   "source": [
    "|| Action0 | Action1 | \n",
    "|:-:|:-:|:-:|\n",
    "|State0 | mean(Reward \\| State0, Action0) |mean(Reward \\| State0, Action1)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ce8a9-6299-4616-9d18-c97d7e6aa816",
   "metadata": {},
   "source": [
    "# 5.  Bandit 환경 설계 및 풀이  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600a5db-10a6-41ad-9dba-6880d33d874d",
   "metadata": {},
   "source": [
    "## A. 대충 개념만 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "3f99cc1b-e046-46d6-be53-2b2f93a4ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [0,1] \n",
    "actions_deque = collections.deque(maxlen=500)\n",
    "rewards_deque =  collections.deque(maxlen=500)\n",
    "#---#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "13146de5-a6ab-4d02-9527-9c09084f374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    action = np.random.choice(action_space)\n",
    "    if action == 1:\n",
    "        reward = 10 \n",
    "    else:\n",
    "        reward = 1\n",
    "    actions_deque.append(action)\n",
    "    rewards_deque.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "67ff5b6a-38aa-4523-ba3f-cca68585be2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([0, 1, 0, 0, 0, 1, 0, 1, 1, 0], maxlen=500)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "6d92dca2-4f94-4468-913f-9df41b190d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([1, 10, 1, 1, 1, 10, 1, 10, 10, 1], maxlen=500)"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "8a83ec00-abff-4f00-9f91-fa2e31c7882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_numpy = np.array(actions_deque)\n",
    "rewards_numpy = np.array(rewards_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "d1d75161-4d30-46bd-9847-f43f0c3fe61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., 10.])"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q0 = rewards_numpy[actions_numpy == 0].mean()\n",
    "q1 = rewards_numpy[actions_numpy == 1].mean()\n",
    "q_table = np.array([q0,q1])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "057398c0-de67-44a3-9b8c-107fdc803857",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = q_table.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "d6f136e0-be0a-4d68-bd95-2de46c107415",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    action = q_table.argmax()\n",
    "    if action == 1:\n",
    "        reward = 10 \n",
    "    else:\n",
    "        reward = 1\n",
    "    actions_deque.append(action)\n",
    "    rewards_deque.append(reward)\n",
    "    actions_numpy = np.array(actions_deque)\n",
    "    rewards_numpy = np.array(rewards_deque)    \n",
    "    q0 = rewards_numpy[actions_numpy == 0].mean()\n",
    "    q1 = rewards_numpy[actions_numpy == 1].mean()\n",
    "    q_table = np.array([q0,q1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "9cd0dfdb-8cd1-4559-beec-35741a29d3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "dd121c26-6028-4a88-b167-f2292ba69786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 10,  1,  1,  1, 10,  1, 10, 10,  1, 10, 10, 10, 10, 10])"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1168d-2f31-4279-9917-d902a5c59c43",
   "metadata": {},
   "source": [
    "## B. 클래스를 이용한 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "4ee96e9f-587c-4358-b728-23718a57d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self):\n",
    "        self.reward = None \n",
    "    def step(self,action):\n",
    "        if action == 0:\n",
    "            self.reward = 1\n",
    "        else: \n",
    "            self.reward = 10 \n",
    "        return self.reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "74991c83-f4ad-4dbb-8e44-4043b557e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Bandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "aa4e545e-50fc-4ede-9ba8-0ddc540e1d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    def act(self):\n",
    "        # 만약에 경험이 20보다 작음 --> 랜덤액션 \n",
    "        # 경험이 20보다 크면 --> action = q_tabel.argmax()\n",
    "        pass \n",
    "    def save_experience(self):\n",
    "        # 데이터 저장 \n",
    "        pass \n",
    "    def learn(self):\n",
    "        # q_table 을 업데이트하는 과정 \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16de50-2ae1-4172-beb1-7acf73fb0fbf",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "앞으로의 수업에서는 아래에 해당하는 클래스의 기본 개념을 숙지하셔야 합니다. (`13wk-2` 주차 강의듣기전까지 꼭!) \n",
    "\n",
    "1. 클래스와 인스턴스의 개념, `__init__`, `self`, 메소드\n",
    "2. 클래스의 상속 \n",
    "\n",
    "관련하여 제가 작년에 수업한 자료는 아래와 같습니다\n",
    "\n",
    "1. <https://guebin.github.io/PP2024/posts/11wk-2.html> 에서 1-7까지.. \n",
    "2. <https://guebin.github.io/PP2024/posts/14wk-2.html> 에서 8-A\n",
    "\n",
    "물론, 꼭 제 강의노트로만 공부하셔야하는것은 아닙니다. 제 수업 외에도 클래스를 잘 설명하는 다양한 자료들이 많이 있으니 자유롭게 참고하여 학습하시기 바랍니다.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
